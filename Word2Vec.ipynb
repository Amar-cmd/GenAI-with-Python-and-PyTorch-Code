{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "mount_file_id": "1WU4ss90e2kpERjBbfCrc4nI2VaHyFPlZ",
      "authorship_tag": "ABX9TyO1wrzSfFvKYA9f0/i6w5W8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amar-cmd/GenAI-with-Python-and-PyTorch-Code/blob/main/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAth import kar rhe hain qki isme log, exp, sqrt etc hota hai...\n",
        "# jo word2vec me kaam aa sakta hai\n",
        "# (loss calculate nd negative sampling ka math)\n",
        "import math\n",
        "\n",
        "# Torch ka use hoga tensor banane (torch.tensor, torch.randn)\n",
        "# GPU pe computation karne ke liye (.to(device))\n",
        "import torch\n",
        "\n",
        "# Yaha neural network banana hai to fir `torch.nn` chahiye\n",
        "# nn use karke mai layers banaunga 'nn.Linear', 'nn.Embedding' etc.\n",
        "# word2vec me mainly nn.Embedding layer use karenge jo wods ko vectors me convert karega\n",
        "import torch.nn as nn\n",
        "\n",
        "# model ko train karne ke liye optimizer (Adam, SGD etc) v chahiye\n",
        "# w2v me v hm har batch ke baad embeddings update karenge optimizer se\n",
        "import torch.optim as optim\n",
        "\n",
        "# Ab data ko handle karna hai\n",
        "# Dataset → ek custom class banayenge jo bataega:\n",
        "#  • total samples (__len__)\n",
        "#  • ith sample (__getitem__)\n",
        "\n",
        "# DataLoader → ye Dataset se data uthata hai:\n",
        "#  • batches banata hai, shuffle krta hai, training loop clean bana deta hai\n",
        "\n",
        "# w2v me hm input word + context/negative samples ko properly batches me\n",
        "# feed karne ke liye use karenge\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# yaha mujhe words ki frequency gin ni hai\n",
        "# Counter - ek list of words lega → frequency of each word return karega\n",
        "# w2v me useful hai:\n",
        "# • vocab banane ke liye\n",
        "# • rare words ko filter karne ke liye\n",
        "# • negative sampling ke probabilities set karne ke liye(frequent words zyada chance, rare ko kam)\n",
        "from collections import Counter\n",
        "\n",
        "# Ab hugging face ka dataset library use karunga\n",
        "# load_dataset se real world text dataset kiad jarege\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "4Av7vFEF6VJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Agnostic Device"
      ],
      "metadata": {
        "id": "O37t2LuF_oKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ab yaha mai check karungna ki kon sa device available hai.\n",
        "torch.cuda.is_available chck karega ki kya mere paas GPU ka access hai ya nhi\n",
        "agar GPU nhi hai to default device CPU ko bana dega nd wha saara kaam hoga\n",
        "'''\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device: \", device)"
      ],
      "metadata": {
        "id": "3BhE0SP9_tfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "vspeKt47iu4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Text Data"
      ],
      "metadata": {
        "id": "I_qPk0UR_2sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "ab mai dataset library ke `load_dataset` ki madad se 2 data lunga.\n",
        "'wikitext; → cleaned version hai\n",
        "'wikitext-2-raw-v1': uncleaned version hai jo original k kaafi close hai\n",
        "                     (includes punctuations, caps etc)\n",
        "'split=train': matlab sirf training wala data load karunga...test wala nhi\n",
        "'''\n",
        "dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n",
        "\n",
        "# ab mujhe text dekhna hai dataset se...isliye mai \"text\" column uthaa lunga\n",
        "lines = dataset[\"text\"]"
      ],
      "metadata": {
        "id": "vkOZoz12_6Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ab word2vec ke liye corpus ready karna hai, let's do it\n",
        "\n",
        "# ek khaali list banaunga jisme words (tokens) jama karunga:\n",
        "# [\"this\", \"is\", \"first\", \"sentence\", \"and\", \"ye\", \"dusra\", \"hai\", ...]\n",
        "tokens = []\n",
        "\n",
        "# ab text ke har ek line ko line-by-line lunga aur...\n",
        "for line in lines:\n",
        "\n",
        "\n",
        "  # uske aage-peeche ke extra spaces ya new line characters hata dunga...\n",
        "  line = line.strip()\n",
        "\n",
        "  # aur jo empty line hai...usse ignore kar dunga\n",
        "  # wikitext me heading kuch aisi hai: '===heading==='\n",
        "  # usse v hata dunga (ignore karke)\n",
        "  if not line or line.startswith('='):\n",
        "    continue\n",
        "\n",
        "  # ab mere paas jo v lines bachi hain...usko mai lower me conver karunga,\n",
        "  # whitespace ke basis pe split kar dunga...\n",
        "  # aur list ke andar isse add kartaa jaaunga.\n",
        "  tokens.extend(line.lower().split())\n",
        "\n",
        "print(f\"Number of tokens = {len(tokens)}\")\n",
        "\n",
        "# To keep training reasonable fast for demo, use only first N tokens\n",
        "max_tokens = 4000000\n",
        "tokens = tokens[:max_tokens]\n",
        "print(\"Total tokens used:\", len(tokens))"
      ],
      "metadata": {
        "id": "_lz_NIsWAT1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Vocabulary"
      ],
      "metadata": {
        "id": "wO9W_RF4BNp3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# av tak hmlg words nikaale hain jo training me dala jaayega.\n",
        "# ab yaha se mai unn words ka ek dict banaunga jo w2v me use hoga\n",
        "\n",
        "# ye ek limit hai. Koi v word 5 baar se kam aaya to usko ignore karenge (rare words)\n",
        "# list me aane ke liye wo 5 ya usse zyada baar repeat hona chahiye\n",
        "min_freq = 5\n",
        "\n",
        "# let's count ki kon sa word kitni baar aaya\n",
        "# { \"the\": 50000, \"of\": 30000, \"data\": 1200, \"india\": 800, ... }\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "\n",
        "# ab mujhe vocab banani hai. sabse phle ek special token rakhunga <unk> ke\n",
        "# naam se, baaki me sirf whi words rakhunga ji 'min_freq' ya usse zyada baar\n",
        "# aaye hain\n",
        "# Result: [\"<unk>\", \"the\", \"of\", \"and\", \"to\", \"in\", ...]\n",
        "vocab = [\"<unk>\"] + [w for w, c in word_counts.items() if c >= min_freq]\n",
        "\n",
        "'''\n",
        "ab mujhe words → indices mapping banana hoga qki model ko words se zyada\n",
        "number pasand hai\n",
        "\n",
        "result = word_to_idx = {\"<unk>\": 0, \"the\": 1, \"of\": 2, \"and\": 3,...}\n",
        "\n",
        "ye mapping important hai.\n",
        "• jab hum koi sentence token list me rakhte hain, to hr word ko uske\n",
        "  index me convert karenge:\n",
        "  \"the\" → 1, \"india\" → maybe 365, unknown/new word → 0 (<unk>)\n",
        "'''\n",
        "word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
        "\n",
        "# ho sakta hai ki index ke basis pe word chahiye hoga...to index to word\n",
        "# ka dictionary v bana leta hu...(debugging, ya nearest neighbour)\n",
        "# Result: ids_to_word = {0: \"<unk>\", 1: \"the\", 2: \"of\", ...}\n",
        "ids_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "\n",
        "# Ab main count kar raha hoon ki final vocabulary me total kitne unique\n",
        "# tokens aaye.\n",
        "# isme unknown v included hai...means: 1 (unk) + total unique words\n",
        "vocab_size = len(vocab)\n",
        "print(f\"Vocabulary size = {vocab_size}\")"
      ],
      "metadata": {
        "id": "6bicdkK4BPjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# yaha se word → number me convert hota hai\n",
        "\n",
        "'''\n",
        "Ab mere paas tokens me saare words sequence me pade hain...lekin model\n",
        "ko words nhi, numbers (indices) chahiye. To chalo har word ko uske\n",
        "index me conver karte hain\n",
        "'''\n",
        "\n",
        "# token se ek word (w) lo, aur dict se uss word ka index nikaalo agar hai to\n",
        "# warna wha 0 daal do (unknown)\n",
        "corpus_indices = [word_to_idx.get(w, 0) for w in tokens]\n",
        "corpus_indices[:10]"
      ],
      "metadata": {
        "id": "XDKRf48rB3SU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset: Skipgram with negative sampling"
      ],
      "metadata": {
        "id": "H64yK0xiCNV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# yaha mai PyTorch Dataset bana raha hu...jo specifically skipgram w2v ke liye hai\n",
        "\n",
        "class SkipGramDataset(Dataset):\n",
        "  def __init__(self, indices, vocab_size, window_size=2, num_negatives=5):\n",
        "    \"\"\"\n",
        "        indices      : poora corpus numbers me: corpus_indices (word ids ki list).\n",
        "        vocab_size   : kitne unique words hain (embedding matrix ka size)\n",
        "        window_size  : center word ke aas-paas kitne context words dekhne hain\n",
        "        num_negatives: har (center, context) ke saath kitne negative words sample karne hain.\n",
        "    \"\"\"\n",
        "    self.indices = indices\n",
        "    self.vocab_size = vocab_size\n",
        "    self.window_size = window_size\n",
        "    self.num_negatives = num_negatives\n",
        "\n",
        "  # BUILD (CENTER, CONTEXT) PAIR\n",
        "  # (center, context) store karne ke liye ek list bana rhe hain\n",
        "    pairs = []\n",
        "\n",
        "    # hm har position wale word ko ek possible center maan rhe hain\n",
        "    # center pos = 0, 1, 2...len(indices) - 1\n",
        "    for center_pos in range(len(indices)):\n",
        "\n",
        "      # iss index pe jo v word hai...wo mera center word hai (assume)\n",
        "      center = indices[center_pos]\n",
        "\n",
        "\n",
        "      # yaha hm sliding window bana rhe hain\n",
        "      # window size 2 hai to...\n",
        "      # offset hoga -2, -1, 0, +1, +2 ← mtlb `2 left, 1 left, khud center, 1 right, 2 right`\n",
        "      for offset in range(-window_size, window_size + 1):\n",
        "\n",
        "\n",
        "        # yaha mera literal context position hai\n",
        "        # 8, 9, 10, 11, 12\n",
        "        context_pos = center_pos + offset\n",
        "\n",
        "        # ye safety check hai out of range exception ka.\n",
        "        # jo v value corpus ke bahar jaa rha hai...usko ignore karo\n",
        "        if context_pos < 0 or context_pos >= len(indices):\n",
        "          continue\n",
        "\n",
        "\n",
        "        # center ka context nhi maan na hai...usko ignore karna hai\n",
        "        if context_pos == center_pos:\n",
        "          continue\n",
        "\n",
        "        # woh neighbor word ka index utha liya.\n",
        "        context = indices[context_pos]\n",
        "\n",
        "        # pair me append kar diye ek tuple of center and context\n",
        "        pairs.append((center, context))\n",
        "\n",
        "\n",
        "    self.pairs = pairs\n",
        "    print(\"Number of (center, context) pairs:\", len(self.pairs))\n",
        "\n",
        "\n",
        "    # BUILD NEGATIVE SAMPLING DISTRIBUTION\n",
        "\n",
        "    # Count how ofter each word index appear\n",
        "    # Result example: {0: 1234, 1: 56789, 2: 1000, ...}\n",
        "    word_freqs = Counter(indices)\n",
        "\n",
        "    # Make a tensor of frequencies with length vocab_size\n",
        "    # Result: [1234, 56789, 1000...] in the form of float tensor\n",
        "    # in case index aaya hi nhi to Counter default value 0 dega\n",
        "    freqs = torch.tensor(\n",
        "        [word_freqs[i] for i in range(vocab_size)],\n",
        "        dtype = torch.float\n",
        "    )\n",
        "\n",
        "    # As in word2vec paper: raise to power 0.75\n",
        "    # Logic: agar word ka freq zyada hai (is, of, the, etc)..to wo distribution ko dominate krta hai\n",
        "    # 0.75 power frequent word ko thoda compress karta hai nd rare word ko thoda chance mil jaata hai\n",
        "    freqs = freqs ** 0.75\n",
        "\n",
        "    # Normalize to make it a probability distribution\n",
        "    # ab ye freqs ko probability me convert karna hai\n",
        "    # isse ek tensor milega jiska sum ~ 1 hoga\n",
        "    self.neg_sampling_dist = freqs / freqs.sum()\n",
        "\n",
        "\n",
        "  # total kitne (center, context) pair hain...wo dekh rha hai\n",
        "  def __len__(self):\n",
        "    return len(self.pairs)\n",
        "\n",
        "  # ith index position wale dataset me value kaisa dikhega...ye pata chalega\n",
        "  def __getitem__(self, idx):\n",
        "    center, context = self.pairs[idx]\n",
        "\n",
        "    # Sample negative word indices from the vocab\n",
        "    # Means...mujhe vocab se num_negatives words sample kar do...\n",
        "    # jaha har word ka prob. neg_sampling_dist se decide ho.\n",
        "    # Replacements allowed hai...means same -ve words diff. times allowed hai\n",
        "    negatives = torch.multinomial(\n",
        "        self.neg_sampling_dist,\n",
        "        self.num_negatives,\n",
        "        replacement = True\n",
        "    )\n",
        "\n",
        "\n",
        "    # ab saare indices ko proper pytorch tensors me bana deta hu...\n",
        "    # taaki directly model me feed ho sake.\n",
        "    # isko long me daale hain qki nn.Embedding ko integer (long) chahiye hote hain\n",
        "    center = torch.tensor(center, dtype=torch.long)\n",
        "    context = torch.tensor(context, dtype=torch.long)\n",
        "    negatives = negatives.long()\n",
        "\n",
        "    # last me (center, context, negatives) ko return karna hai\n",
        "    return center, context, negatives"
      ],
      "metadata": {
        "id": "57yjh0-DCRvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset and dataloader\n",
        "dataset = SkipGramDataset(corpus_indices, # poore corpus ka integer sequence [12, 45, 7, 89, ...]\n",
        "\n",
        "                          # kitne unique words hai\n",
        "                          # negative sampling ke liye frequency tensor ka size decide hoga\n",
        "                          # prob. dist. (-ve sampling dist) ka shape set hoga\n",
        "                          vocab_size=vocab_size,\n",
        "\n",
        "                          # center word ke left-right 2 words context ke liye jaayenge\n",
        "                          # ctr = brown, ctx = [the, quick, fox, jumps]\n",
        "                          window_size=2,\n",
        "\n",
        "                          # har (ctr, ctx) positive pair ke saath 5 random -ve words aayenge training sample se\n",
        "                          num_negatives=5)\n",
        "\n",
        "dataloader = DataLoader(dataset, # dataset se items uthata hai\n",
        "\n",
        "                        # 512 samples milega...\n",
        "                        # 512 center words\n",
        "                        # 512 positive context words\n",
        "                        # 512 x 5 negative words\n",
        "                        batch_size=512,\n",
        "\n",
        "                        # dataset ke indices ko reshuffle kar do\n",
        "                        shuffle=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "BvJjtKV-91FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL: WORD2VEC SKIPGRAM + NEGATIVE SAMPLING\n",
        "class Word2VecSkipGram(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size, # kitne words hain\n",
        "               embed_dim): # har word ka vector kitne dimension ka hoga (e.g. 100, 300)\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    '''\n",
        "    Har word kv center banta hai aur kv context.\n",
        "    w2v me in dono roles ke liye alag embeddings rakhe jaate hain.\n",
        "    '''\n",
        "    # embeddings when word acts as the center\n",
        "    self.in_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "    # embeddings when word acts as the context\n",
        "    self.out_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "    # ab embeddings ko initial value deke initialize karna hai\n",
        "\n",
        "    # ye chhota sa number hai. agar embed_dim 100 hai...\n",
        "    # to initial range [-0.005, 0.005] ke beech hoga\n",
        "    init_range = 0.5 / embed_dim\n",
        "\n",
        "    # center embeddings ko chhoti random values de rhe hain.\n",
        "    # agar sab 0 se start hua to saare word initially same honge...jo ki nhi hai\n",
        "    # random init se har word thoda alag start karta hai.\n",
        "    self.in_embed.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "\n",
        "    # context embeddings ko zero se init kar rahe ho.\n",
        "    # grad aayega to update hota rahega\n",
        "    self.out_embed.weight.data.zero_()\n",
        "\n",
        "    # ye design choice hai...in embed ko randm nd output embed ko 0, gradually learn\n",
        "\n",
        "\n",
        "\n",
        "  # forward me hm directly loss compute karenge\n",
        "  # ye typical classifier jaisa logits return nhi krega\n",
        "  # yehi pe sigmoid + log + loss sab ho jaayega\n",
        "  def forward(self, center_words, pos_context, neg_context):\n",
        "      \"\"\"\n",
        "      center_words : (batch,) → har entry ek word index hai\n",
        "      pos_context  : (batch,) → har center ke liye ek +ve context word index\n",
        "      neg_context  : (batch, num_negatives) → har center ke liye k -ve word indices\n",
        "      \"\"\"\n",
        "\n",
        "      # Lookup embeddings\n",
        "      # (batch, num_negatives, embed_dim) == (B, K, D)\n",
        "      center_emb = self.in_embed(center_words) #(batch,) → (batch, embed_dim)\n",
        "      pos_emb = self.out_embed(pos_context) # (batch, ) → (batch, embed_dim)\n",
        "      neg_emb = self.out_embed(neg_context) # (batch, num_negatives) → (batch, num_negatives, embed_dim)\n",
        "\n",
        "\n",
        "      # POSITIVE PART → “ye real pair hai, score high karo”\n",
        "      # center • positive context (element wise multiply)\n",
        "      # dot product ke baad dim=1 (embedding ke features) ka sum kar do; dim=0 batch hai\n",
        "      pos_score = torch.sum(center_emb * pos_emb, dim=1)\n",
        "\n",
        "      # We want sigmoid(pos_score) close to 1\n",
        "      # sigmoid ko [0,1] probability me convert karna hai:\n",
        "      # 1) high dot product → sigmoid ≈ 1\n",
        "      # 2) low dot product → sigmoid ≈ 0\n",
        "      # + 1e-10: for stability → kahi sigmoid bilkul 0 ho jaaye to log(0) na ho.\n",
        "      pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # NEGATIVE PART\n",
        "\n",
        "      # center_emb: (batch, embed_dim) -> (batch, embed_dim, 1)\n",
        "      # neg_emb   : (batch, num_neg, embed_dim)\n",
        "      # yaha .unsqueeze(2) karne se center_emb nd neg_embed dono same dimension ka hoga → matrix multiplication possible → no shape mismatch error\n",
        "\n",
        "      # bmm = match matrix multiplication\n",
        "      # har batch ke liye\n",
        "      # • neg_emb[b] shape: (K, D)\n",
        "      # • center_emb[b] shape: (D, 1)\n",
        "      # • result: (K, 1) = har -ve word ja dot product center ke saath\n",
        "      # Overall result (including batch also) → (B, K, 1)\n",
        "      # .unsqueeze() karke last dim (1) ho hata do → (B, K)\n",
        "      neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze()\n",
        "\n",
        "\n",
        "\n",
        "      # We want sigmoid(neg_score) close to 0\n",
        "      # yaha 1-sigmoid(neg_score) ~ 1\n",
        "      # log(1-sigmoid(...)) ~ 0\n",
        "      # .sum() isliye qki har example me multiple -ve words hain (k negatives)\n",
        "      # unn sab ke losses ko sum karo and last me total -ve loss per center de do.\n",
        "      neg_loss = -torch.sum(\n",
        "          torch.log(1 - torch.sigmoid(neg_score) + 1e-10),\n",
        "          dim=1\n",
        "      )\n",
        "\n",
        "      # combine and average over batch\n",
        "      loss = (pos_loss + neg_loss).mean()\n",
        "\n",
        "      return loss"
      ],
      "metadata": {
        "id": "5zvZy0HC-OlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "# embedding dimension decide kare hain...har word ko 100 length ke vector se represent karenge\n",
        "embed_dim = 50 # 200, 300...are common\n",
        "\n",
        "\n",
        "# ab model bana ke usko sahi device pe send kr rahe hain (CPU/GPU)\n",
        "# w2v do embedding matrix banayega:\n",
        "# • in_embed = (vocab_size, embed_dim)\n",
        "# • out_embed = (vocab_size, embed_dim)\n",
        "model = Word2VecSkipGram(vocab_size, embed_dim).to(device)\n",
        "\n",
        "# adam adaptive optimizer hai...lr ko har param ke liye smart tareeke se adjust karta hai\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "\n",
        "# mai poora dataset model ko 10 baar dikhaaunga\n",
        "num_epochs = 1\n",
        "\n",
        "\n",
        "# for each epoch from 0 - 49...\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # training mode me jaao\n",
        "  # agar batchnorm ya dropout hota to unka behaviour change ho jaata\n",
        "  # lekin iss case me nhi hai to as such change nhi hoga\n",
        "  model.train()\n",
        "\n",
        "  # total loss ko start me 0 se initialize karo\n",
        "  total_loss = 0.0\n",
        "\n",
        "\n",
        "  # yaha ctr, ctx, neg ko sahi device pe send karo jaha data available hai\n",
        "  # warna error milega...expected device cuda but got gpu\n",
        "  for center, context, negatives in dataloader:\n",
        "    center = center.to(device)\n",
        "    context = context.to(device)\n",
        "    negatives = negatives.to(device)\n",
        "\n",
        "    # pichle batch ke gradients ko phle clear kar do\n",
        "    # pytorch me gradients by default accumulate hote hain...\n",
        "    # agar ye nhi kara to backprop prev. gradient ke upar add ho jaayega\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # ab forward pass karo:\n",
        "    # • embeddings lookup\n",
        "    # • positive score\n",
        "    # • negative score\n",
        "    # • pos_loss, neg_loss\n",
        "    # • combine → mean\n",
        "    # loss ek scalar tensor hoga (ek avg value)\n",
        "    loss = model(center, context, negatives)\n",
        "\n",
        "    # ab loss pe backprop karo nd loss ke hisab se har wt ka gradient nikaal do.\n",
        "    loss.backward()\n",
        "\n",
        "    # ab gradient ke hisaab se weight ko update karo\n",
        "    # Adam → `param = param - lr * (processed_gradient)`\n",
        "    optimizer.step()\n",
        "\n",
        "    # iss epoch ke liye har batches ka total loss add karo\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}/{num_epochs}, loss = {total_loss: .4f}\")"
      ],
      "metadata": {
        "id": "v1tIbV_rA6MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the learned embeddings: find similar words\n",
        "\n",
        "# yaha sirf inference kar rhe hain...training nhi\n",
        "@torch.no_grad()\n",
        "\n",
        "# ek word ko embedding vector me convert karna hai\n",
        "def get_word_embedding(word: str) -> torch.Tensor:\n",
        "\n",
        "  # phle word ka index nikaal lo (from available vocab dict)\n",
        "  idx = word_to_idx.get(word, 0)\n",
        "\n",
        "  # ab iss index wale row ka weight nikaal lunga in_embed se...jo model me hai,\n",
        "  # aur usse return kar dunga\n",
        "  return model.in_embed.weight[idx]\n",
        "\n",
        "\n",
        "# again inference mode hai, grad nhi chahiye\n",
        "@torch.no_grad()\n",
        "\n",
        "# ek word ke top-k similar word predict karna hai\n",
        "def most_similar(query_word: str, top_k: int = 5):\n",
        "\n",
        "  # agar word vocab me nhi hai to bata do ki similar word predict nhi ho paayega\n",
        "  # Ab main us word ka actual embedding vector le raha hoon.\n",
        "  if query_word not in word_to_idx:\n",
        "    print(f\"'{query_word}' is not in the vocab.\")\n",
        "    return\n",
        "\n",
        "  # query word ka embedding lo jo ek tensor hoga...wo store kar lo\n",
        "  # shape: (embed_dim,)\n",
        "  query_vec = get_word_embedding(query_word)\n",
        "\n",
        "  # iss all_embs me saare words ke embeddings ek saath aa gaye\n",
        "  # Shape: (vocab_size, embed_dim)\n",
        "  all_embs = model.in_embed.weight\n",
        "\n",
        "  # Cosine similarity: (v . w / (|v||w|))\n",
        "  # Shape: (vocab_size, embed_dim) • (embed_dim,) = (vocab_size,)\n",
        "  # all_emb.norm(...) = har word embedding ki L2 norm\n",
        "  # query_vec.norm(...) = query word ki L2 norm\n",
        "  # L2 norm = Euclidean norm = ||x||₂ = √(x₁² + x₂² + ... + xₙ²)\n",
        "  #   dim=0 → “kitne examples / words hain” ka axis\n",
        "  # dim=1 → “har example ka vector / features” ka axis\n",
        "  sims = torch.matmul(all_embs, query_vec) / (\n",
        "      all_embs.norm(dim=1) * query_vec.norm() + 1e-10\n",
        "  )\n",
        "\n",
        "  values, indices = torch.topk(sims, top_k + 1) # +1 to include the word itself, will skip it later\n",
        "  print(f\"\\nWords most similar to '{query_word}':\")\n",
        "\n",
        "  # yaha topk+1 similarity ke liye (score, idx) use kro and uspe iterate karo\n",
        "  for score, idx in zip(values, indices):\n",
        "\n",
        "    # index ki madad se uska word nikaal lo (ids_to_word) dict ka use karke\n",
        "    w = ids_to_word[idx.item()]\n",
        "\n",
        "    # Agar query word khud word me aaye to usse ignore karo\n",
        "    if w == query_word:\n",
        "      continue\n",
        "    print(f\"{w:15s} similarity = {score.item(): .3f}\")"
      ],
      "metadata": {
        "id": "-D6VVX72HCAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar(\"king\")"
      ],
      "metadata": {
        "id": "7Koq0b-lRUj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar(\"queen\")"
      ],
      "metadata": {
        "id": "9eLl5S-fRYuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar(\"london\")"
      ],
      "metadata": {
        "id": "fQpTxJ4gRaJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1nzXi-WSFSlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/\"Colab Notebooks\"/\"GenAI with Python and PyTorch\"/\"Chapter 3\""
      ],
      "metadata": {
        "id": "V-fg4PZGFWgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"word2vec_state.pth\")"
      ],
      "metadata": {
        "id": "2p3K638HFY-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Page 70 (from book) - Using gensim library\n"
      ],
      "metadata": {
        "id": "LbhnV7LJ6SDt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ql_3BUBNEGR"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "CdT0YXYgNIki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "bz_mSsafNIhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_document(doc):\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "\n",
        "  tokens = nltk.word_tokenize(doc)\n",
        "\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "\n",
        "  return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "metadata": {
        "id": "7iazZR1BNIe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cats = ['alt.atheism', 'sci.space']\n",
        "newsgroup_train = fetch_20newsgroups(subset='train',\n",
        "                                     categories=cats,\n",
        "                                     remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "zgRwrp27NIcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of news articles = {}'.format(len(newsgroup_train.data)))"
      ],
      "metadata": {
        "id": "W-tF4GL5NIZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_corpus = normalize_corpus(newsgroup_train.data)\n",
        "norm_corpus"
      ],
      "metadata": {
        "id": "SzBVF-4ENIWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "2QqMgDcVNITb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import word2vec"
      ],
      "metadata": {
        "id": "FgNgY9_0NIQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]"
      ],
      "metadata": {
        "id": "FTPOXVZgNIOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size = 32\n",
        "context_window = 20\n",
        "min_word_count = 1\n",
        "sample = 1e-3\n",
        "sg = 1\n",
        "\n",
        "w2v_model = word2vec.Word2Vec(tokenize_corpus,\n",
        "                              vector_size = embedding_size,\n",
        "                              window=context_window,\n",
        "                              min_count = min_word_count,\n",
        "                              sg = sg,\n",
        "                              sample=sample,\n",
        "                              epochs=200)"
      ],
      "metadata": {
        "id": "JVi5QOBeNILT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique numbers of words in the model={w2v_model.wv.vectors.shape[0]}\")"
      ],
      "metadata": {
        "id": "afb2c8ZuNIIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv['sun']"
      ],
      "metadata": {
        "id": "hGOBuEsHNIEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=['god'])"
      ],
      "metadata": {
        "id": "EWYuTeVkNH4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.most_similar(positive=['sun'])"
      ],
      "metadata": {
        "id": "E3CAvdPaNWhc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}