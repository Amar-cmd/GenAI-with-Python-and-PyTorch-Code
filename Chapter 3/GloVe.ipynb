{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"18Hrg3NFmYXJQohwB4-OVD7seFs2nOo6J","authorship_tag":"ABX9TyNcbAKdESzxRiGXGF1pN4pm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# GloVe Implementation\n","\n","GloVe = **G**lobal **Ve**ctors for word representation.\n","\n","Goal:\n","Learn a vector for each word so that:\n","\n","* Similar words ⇒ similar vectors\n","* Word relationships (like *king - man + woman ≈ queen*) show up as vector arithmetic\n","\n","How it’s different from word2vec:\n","\n","* **word2vec** learns from *local* context (sliding windows, predicting neighbors).\n","* **GloVe** learns from **global co-occurrence counts**:\n","  “How often does word *i* appear near word *j* in the whole corpus?”"],"metadata":{"id":"ur6h6tW_Ag-Y"}},{"cell_type":"code","source":["# math isliye qki log, sqrt, exp etc ka zaroorat hoga.\n","# Co-occurance counts pe log lagaya jaata hai (bhot bade numbers ko compress karne k liye)\n","import math\n","\n","# Har words ki freq chahiye vocab banane ke liye\n","# default dict: agar koi aise key ko access karna chahe jo exist nhi karta to uske liye ye ek default value bana deta hai\n","from collections import Counter, defaultdict\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n"],"metadata":{"id":"SefhFvRdA268","executionInfo":{"status":"ok","timestamp":1764006588662,"user_tz":-330,"elapsed":3712,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","    \"king queen man woman\",\n","    \"king man strong\",\n","    \"queen woman kind\",\n","    \"man woman child\",\n","    \"king queen royal family\",\n","    \"queen royal palace\",\n","    \"man king leader\",\n","]\n"],"metadata":{"id":"-CRNVb1mBB9t","executionInfo":{"status":"ok","timestamp":1764006588697,"user_tz":-330,"elapsed":4,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Tokenize and build vocab\n","\n","# Very basic: text ko lower me convert karo + space ke basis pe split karo\n","def tokenize(text):\n","  return text.lower().split()\n","\n","# Har Sentence ke word level pe tokenize kar raha hai.\n","# Eg: corpus = [\"I love deep learning\", \"GloVe is cool\"]\n","# Result: [[\"i\", \"love\", \"deep\", \"learning\"], [\"glove\", \"is\", \"cool\"]]\n","tokenized_corpus = [tokenize(sentence) for sentence in corpus]\n","\n","\n","# Build Vocab\n","# sentence ke corpus me se ek sentence pakdo, uss ek sentence ka ek word pakdo, fir uska frequency batao\n","# result: {\"i\": 3, \"love\": 2, \"deep\": 1, ...}\n","word_counts = Counter(word for sent in tokenized_corpus for word in sent)\n","\n","# Ab is word count ke keys (words) ko pick karo and usse sort kar do\n","vocab = sorted(word_counts.keys())\n","\n","# Sorted dict ke index and word ko lo and usse \"word, index\" ke form me store karo\n","# word ko number form me represent karne me kaam aayega\n","word2id = {w: i for i, w in enumerate(vocab)}\n","\n","# yaha \"index, word\" format me store karo\n","# ya index ke basis pe word ko find krne me kaam aayega (evaluation time)\n","id2word = {i: w for w, i in word2id.items()}\n","\n","vocab_size = len(vocab)\n","print(\"Vocab: \", vocab)\n","print(\"Vocab size: \", vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VftiXUr6A993","executionInfo":{"status":"ok","timestamp":1764006588711,"user_tz":-330,"elapsed":10,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"1556cb8f-f0d1-4510-f6b9-914b4d9492c3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab:  ['child', 'family', 'kind', 'king', 'leader', 'man', 'palace', 'queen', 'royal', 'strong', 'woman']\n","Vocab size:  11\n"]}]},{"cell_type":"code","source":["# Build co-occurance matrix\n","''' :For each position i in a sentence, look at words within a window\n"," (say 2) around it and count those as co-occurrences.\n","'''\n","\n","# tokenized_corpus = [['king', 'queen', 'man', 'woman'], ['king', 'man', 'strong']]\n","# word2idx = \"word, index\" ki mapping wali dict\n","# window_size = center word ke left/right kitne words pick karne hai as context words\n","def build_cooccurance(tokenized_corpus, word2id, window_size=2):\n","\n","  # {(center_id, context_id): coocurance count} wali dict\n","  cooccurance_matrix = defaultdict(float)\n","\n","  # tokenized corpus ke har ek sentence ko pick karo...\n","  # Eg: ['king', 'queen', 'man', 'woman'] ← Sentence 1\n","  for sentence in tokenized_corpus:\n","\n","    # uss word ke corrosponding jo id hai wo pick karo...\n","    # Eg: queen -> 1\n","    ids = [word2id[word] for word in sentence]\n","\n","    # ab har id and uska index pick karo aur fir...\n","    # Eg: (0, 1)\n","    for center_idx, center_id in enumerate(ids):\n","\n","      # look at context words in [center - window size, center + window size]\n","      # window size ka start pick karo, 0 ya fir center - window size...jo bada hai wo le lo. Ye ensure karega ki -ve index wale words pick naa ho...warna wo error create karenge\n","      start = max(0, center_idx - window_size)\n","\n","\n","      # sentence ka last id ya fir center + window size...jo v chhota hai wo pick karo. Chhota isliye qki bada id lenge to out of range ka issue aa jaayega.\n","      end = min(len(ids), center_idx + window_size + 1)\n","\n","      # ab mere paas start v hai and end v hai...nd ye mera context index (window) hai\n","      # mai har ek context id pick karunga and aage badhunga...\n","      for ctx_idx in range(start, end):\n","\n","        # agar context id aur center id same hai...to ignore karo\n","        if ctx_idx == center_idx:\n","          continue\n","\n","        # ab mere paas range aa gaya hai context window ka.\n","        # uss range ka use karke asli context id pick karo ids se.\n","        context_id = ids[ctx_idx]\n","\n","        # optionally weight the distance (closer words = stronger)\n","        # center index nd context index k beech ka distance calc. kro\n","        # aur 1 se divide karke pata chalega (normalized hoke) ki word ka weight kya hai.\n","        distance = abs(center_idx - ctx_idx)\n","        weight = 1.0 / distance\n","\n","        # key: (center id, context id) & value: existing count + weight\n","        # agli baar agar pair dobaara mila to gradually weight accumulate hoga\n","        cooccurance_matrix[(center_id, context_id)] += weight\n","\n","  return cooccurance_matrix\n","\n","cooccurance_matrix = build_cooccurance(tokenized_corpus, word2id, window_size=2)\n","print(\"Number of co-occurring pairs:\", len(cooccurance_matrix))\n","print('(i, j) -> count\\n')\n","for key, value in cooccurance_matrix.items():\n","  print(key, ' -> ', value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEGXoo4pDJYq","executionInfo":{"status":"ok","timestamp":1764006588719,"user_tz":-330,"elapsed":6,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"a173e482-fcb2-46df-dbb6-6de8bdbd7111"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of co-occurring pairs: 38\n","(i, j) -> count\n","\n","(3, 7)  ->  2.0\n","(3, 5)  ->  2.5\n","(7, 3)  ->  2.0\n","(7, 5)  ->  1.0\n","(7, 10)  ->  1.5\n","(5, 3)  ->  2.5\n","(5, 7)  ->  1.0\n","(5, 10)  ->  2.0\n","(10, 7)  ->  1.5\n","(10, 5)  ->  2.0\n","(3, 9)  ->  0.5\n","(5, 9)  ->  1.0\n","(9, 3)  ->  0.5\n","(9, 5)  ->  1.0\n","(7, 2)  ->  0.5\n","(10, 2)  ->  1.0\n","(2, 7)  ->  0.5\n","(2, 10)  ->  1.0\n","(5, 0)  ->  0.5\n","(10, 0)  ->  1.0\n","(0, 5)  ->  0.5\n","(0, 10)  ->  1.0\n","(3, 8)  ->  0.5\n","(7, 8)  ->  2.0\n","(7, 1)  ->  0.5\n","(8, 3)  ->  0.5\n","(8, 7)  ->  2.0\n","(8, 1)  ->  1.0\n","(1, 7)  ->  0.5\n","(1, 8)  ->  1.0\n","(7, 6)  ->  0.5\n","(8, 6)  ->  1.0\n","(6, 7)  ->  0.5\n","(6, 8)  ->  1.0\n","(5, 4)  ->  0.5\n","(3, 4)  ->  1.0\n","(4, 5)  ->  0.5\n","(4, 3)  ->  1.0\n"]}]},{"cell_type":"code","source":["# Turn co-occurance dict into a dataset\n","\n","# (Dataset) bata rha hai ki hm apna custom dataset bana rhe hain\n","# (i, j, xij) ka pair hoga...as data\n","class CooccuranceDataset(Dataset):\n","\n","  # jab ye dataset banega to sbse phle ye givn code run hoga.\n","  # ye (i, j, xij) ka pair banayega nd usko data object me store kr lega\n","  def __init__(self, cooc_dict):\n","    self.data = [(i, j, xij) for (i, j), xij in cooc_dict.items()]\n","\n","  # poore dataset ka size (length) batayega\n","  def __len__(self):\n","    return len(self.data)\n","\n","  # specific index ka corrosponding i, j, xij nikaal ne ka feature\n","  # ye tensor ke form me return karenge\n","  def __getitem__(self, idx):\n","    i, j, xij = self.data[idx]\n","    return (\n","            torch.tensor(i, dtype=torch.long),\n","            torch.tensor(j, dtype=torch.long),\n","            torch.tensor(xij, dtype=torch.float32),\n","        )\n","\n","dataset = CooccuranceDataset(cooccurance_matrix)\n","\n","\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"],"metadata":{"id":"Td1k3SlHHAJ-","executionInfo":{"status":"ok","timestamp":1764006593069,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Define GloVe Model\n","class GloVe(nn.Module):\n","\n","  # yaha hm unique words (vocab size), and har word ka vector (embed_dim) le rhe hain\n","  def __init__(self, vocab_size, embedding_dim):\n","\n","    # ab jaise hi ye class run hoga...mere paas vocab size nd embed size immediately aa jaayega\n","    super().__init__()\n","    self.vocab_size = vocab_size\n","    self.embedding_dim = embedding_dim\n","\n","    # Word and context embedding\n","    # ab hm word nd context ka embedding banayenge (vocab size nd embed dim ke basis pe)\n","    self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","    self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n","\n","    # Biases\n","    # yaha hm bias banayenge. Isme sirf vocab size hoga nd sirf 1 hi embed dim hoga\n","    self.word_biases = nn.Embedding(vocab_size, 1)\n","    self.context_biases = nn.Embedding(vocab_size, 1)\n","\n","    # Initialize reasonable small\n","    # xavier_uniform_ standard weight init hai jo gradient ko stable rakhta hai\n","    # Random chhote values deta hai aise range me jo layer ke size pe depend karta hai\n","    nn.init.xavier_uniform_(self.word_embeddings.weight)\n","    nn.init.xavier_uniform_(self.context_embeddings.weight)\n","\n","    # bias ke initial values 0 rakho.\n","    nn.init.zeros_(self.word_biases.weight)\n","    nn.init.zeros_(self.context_biases.weight)\n","\n","  def forward(self, i_indices, j_indices, x_ij, x_max=100.0, alpha=0.75):\n","    \"\"\"\n","      i_indices: batch of word ids (center)\n","      j_indices: batch of word ids (context)\n","      x_ij: batch of co-occurrence counts\n","      x_max: threshold for weighting function\n","      alpha: weighting exponent\n","    \"\"\"\n","\n","    # yaha hm word embedding se ith index ka row ka weight nikaal rhe hain\n","    w_i = self.word_embeddings(i_indices)            # (batch, dim)\n","    w_j = self.context_embeddings(j_indices)         # (batch, dim)\n","    b_i = self.word_biases(i_indices).squeeze(-1)    # (batch,)\n","    b_j = self.context_biases(j_indices).squeeze(-1) # (batch,)\n","\n","    # Dot product w_i^T w_j\n","    dot = (w_i * w_j).sum(dim=1)  # (batch,)\n","\n","    # log(X_ij)\n","    log_x_ij = torch.log(x_ij)\n","\n","    # Weighting function f(x)\n","    # f(x) = (x/x_max)^alpha if x < x_max else 1\n","    # x_ij < x_max...isse 0-1 ke beech value rahega\n","    # torch.pow(...) smooth curve bana dega...chhote x pe bhot chhota value\n","    weight = torch.pow(x_ij / x_max, alpha)\n","\n","    # agar x_ij > x_max ke to pow ke wajah se value >1 ho sakta hai.\n","    # Isliye isko clamp kr diye 1 pe...\n","    weight = torch.clamp(weight, max=1.0)\n","\n","    # GloVe loss for this batch (vector)\n","    # loss_term_(ij)​=f(X_ij​)⋅(error_ij​)^2\n","    loss_terms = weight * torch.pow(dot + b_i + b_j - log_x_ij, 2)\n","\n","     # Mean over batch is fine (original is sum but scale doesn’t matter much)\n","    # • Paper me to pure corpus pe sum hota hai\n","    # • Lekin training me hm batch by batch kar rahe to mean vs sum ka farak sirf scale ka hota hai\n","    # • Mean zyada stable hota hai\n","    # • 0.5* because L2 loss ka standard form: 0.5*(Error)^2\n","    loss = 0.5 * torch.mean(loss_terms)\n","    return loss\n","\n","  def get_word_vectors(self):\n","    # Often, people use w + w̃ as the final embedding\n","    return self.word_embeddings.weight.data + self.context_embeddings.weight.data"],"metadata":{"id":"hPCmmn3lOHH9","executionInfo":{"status":"ok","timestamp":1764006595076,"user_tz":-330,"elapsed":24,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","embedding_dim = 50\n","model = GloVe(vocab_size, embedding_dim)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n","\n","num_epochs = 100\n","\n","for epoch in range(1, num_epochs + 1):\n","    total_loss = 0.0\n","    for i_indices, j_indices, x_ij in dataloader:\n","        optimizer.zero_grad()\n","        loss = model(i_indices, j_indices, x_ij)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item() * len(x_ij)\n","\n","    avg_loss = total_loss / len(dataset)\n","    if epoch % 10 == 0 or epoch == 1:\n","        print(f\"Epoch {epoch}/{num_epochs} - Loss: {avg_loss:.4f}\")"],"metadata":{"id":"17n5-zAVQJQR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764006603181,"user_tz":-330,"elapsed":5284,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"b1ec6dbb-b40a-42be-ee0a-cf7c762fef72"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100 - Loss: 0.0079\n","Epoch 10/100 - Loss: 0.0005\n","Epoch 20/100 - Loss: 0.0001\n","Epoch 30/100 - Loss: 0.0001\n","Epoch 40/100 - Loss: 0.0000\n","Epoch 50/100 - Loss: 0.0000\n","Epoch 60/100 - Loss: 0.0000\n","Epoch 70/100 - Loss: 0.0000\n","Epoch 80/100 - Loss: 0.0000\n","Epoch 90/100 - Loss: 0.0000\n","Epoch 100/100 - Loss: 0.0000\n"]}]},{"cell_type":"code","source":["# Inspect the learned embeddings\n","\n","import torch.nn.functional as F\n","\n","def most_similar(query_word, model, word2id, id2word, top_k=5):\n","    if query_word not in word2id:\n","        print(f\"Word '{query_word}' not in vocabulary\")\n","        return\n","\n","    word_vectors = model.get_word_vectors()  # (vocab_size, dim)\n","    word_vectors = F.normalize(word_vectors, dim=1)\n","\n","    query_id = word2id[query_word]\n","    query_vec = word_vectors[query_id].unsqueeze(0)  # (1, dim)\n","\n","    # Cosine similarity with all words\n","    similarities = torch.mm(query_vec, word_vectors.t()).squeeze(0)  # (vocab_size,)\n","\n","    # Get top_k+1 because first will be the word itself\n","    sim_values, sim_indices = torch.topk(similarities, top_k + 1)\n","\n","    print(f\"Most similar to '{query_word}':\")\n","    for score, idx in zip(sim_values, sim_indices):\n","        w = id2word[idx.item()]\n","        if w == query_word:\n","            continue\n","        print(f\"  {w:10s}  (cosine similarity: {score.item():.3f})\")\n"],"metadata":{"id":"oJmBFRyXQn-i","executionInfo":{"status":"ok","timestamp":1764006612466,"user_tz":-330,"elapsed":16,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["most_similar(\"king\", model, word2id, id2word, top_k=5)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mNBNfIPQ16J","executionInfo":{"status":"ok","timestamp":1764006614522,"user_tz":-330,"elapsed":47,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"3be7ba6d-2c46-495a-f24c-11c509d3765a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar to 'king':\n","  man         (cosine similarity: 0.792)\n","  woman       (cosine similarity: 0.454)\n","  queen       (cosine similarity: 0.315)\n","  palace      (cosine similarity: 0.009)\n","  kind        (cosine similarity: -0.159)\n"]}]},{"cell_type":"code","source":["most_similar(\"queen\", model, word2id, id2word, top_k=5)\n"],"metadata":{"id":"bE88I_njQ5xI","executionInfo":{"status":"ok","timestamp":1764006623541,"user_tz":-330,"elapsed":42,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"7161fca6-d85c-4485-f04f-6994f5ad2ca7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Most similar to 'queen':\n","  man         (cosine similarity: 0.407)\n","  woman       (cosine similarity: 0.325)\n","  king        (cosine similarity: 0.315)\n","  royal       (cosine similarity: 0.208)\n","  child       (cosine similarity: -0.163)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X0ZknX4u_Q6d","executionInfo":{"status":"ok","timestamp":1764007616166,"user_tz":-330,"elapsed":4960,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"179fec5c-db63-4a6b-9b5b-a594a9d1ab89"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/\"Colab Notebooks\"/\"GenAI with Python and PyTorch\"/\"Chapter 3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N3X1_oogBkTd","executionInfo":{"status":"ok","timestamp":1764007947121,"user_tz":-330,"elapsed":34,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"1662ca6f-747d-4d24-dd17-797a656093bb"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/GenAI with Python and PyTorch/Chapter 3\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"glove_state.pth\")"],"metadata":{"id":"abCMEkWeADdB","executionInfo":{"status":"ok","timestamp":1764007948304,"user_tz":-330,"elapsed":7,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"osxGp9-REBDU"},"execution_count":null,"outputs":[]}]}