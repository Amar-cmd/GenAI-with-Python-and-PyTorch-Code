{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOpcxSl7HkM0J1Dk3o3HMLu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amar-cmd/GenAI-with-Python-and-PyTorch-Code/blob/main/Chapter%203/FastText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.1 FastText kya hai?**\n",
        "\n",
        "FastText Facebook AI ka model hai jo mainly **text classification + word embeddings** ke liye use hota hai.\n",
        "Basic idea:\n",
        "\n",
        "* Har word ko sirf ek token na maan kar, **character n-grams** ka bag banaya jata hai.\n",
        "\n",
        "  * Example: word = `where`\n",
        "  * Hum boundary add karte hain: `<where>`\n",
        "  * 3-gram: `<wh`, `whe`, `her`, `ere`\n",
        "  * 4-gram: `<whe`, `wher`, `here` …\n",
        "* Har n-gram ka apna embedding hota hai.\n",
        "* Word ka embedding = uske saare n-grams ka average/sum.\n",
        "\n",
        "Isse fayda:\n",
        "\n",
        "* **OOV (out-of-vocabulary)** words bhi handle ho jate hain (kyunki unke characters toh dekhe ja sakte hain).\n",
        "* Morphology (prefix/suffix) bhi capture hoti hai (e.g., play, player, playing).\n",
        "\n",
        "# **1.2 FastText for classification (jo hum implement karenge)**\n",
        "\n",
        "Supervised FastText classifier roughly yeh karta hai:\n",
        "\n",
        "1. Input: Ek sentence / document ka text.\n",
        "2. Preprocess:\n",
        "\n",
        "   * Lowercase, punctuation remove, simple tokenization (split by spaces).\n",
        "3. Har word → **char n-grams** + ek special “whole word” n-gram.\n",
        "4. Har n-gram ko ek **integer id** se map karte hain (hum yahan hashing trick use karenge).\n",
        "5. Sentence ke sabhi n-grams ke embeddings ka **average** nikalte hain (bag-of-ngrams).\n",
        "6. Yeh average vector ek **linear layer + softmax** mein jata hai → class probabilities.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "* Sentence text ke liye n-gram ids: ({g_1, g_2, \\dots, g_M})\n",
        "* Embedding matrix (E \\in \\mathbb{R}^{V \\times d}), jahan (V) = bucket size (hashed vocab size), (d) = embedding dim.\n",
        "\n",
        "Sentence vector:\n",
        "$$\n",
        "v = \\frac{1}{M} \\sum_{i=1}^M E[g_i]\n",
        "$$\n",
        "\n",
        "Phir classifier:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(W v + b)\n",
        "$$\n",
        "\n",
        "Loss: standard **cross-entropy** loss between predicted distribution and true label.\n"
      ],
      "metadata": {
        "id": "09wl4MjQLkWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "X06s5rsElSLq",
        "outputId": "ceb2bdea-a169-493f-915e-d66112804344"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to clean raw text (remove punct, lowercase etc)\n",
        "# python ka re module regex ke liye best hai\n",
        "import re\n",
        "\n",
        "# FastText ke liye har character n-gram ko hash karna hoga\n",
        "# To hmko ek deterministic hash chahiye...jo string -> integer de.\n",
        "import hashlib\n",
        "\n",
        "# Reproducality impt hai (same shuffling, same initialisation etc.)\n",
        "# Isliye global python RNG ko seend krne k liye 'random' chahiye\n",
        "import random\n",
        "\n",
        "# For implementation of code using pytorch, torch is needed\n",
        "import torch\n",
        "\n",
        "# 'nn' module se neural network building blocks milte hain\n",
        "# Linear Layer, Embedding, Losses etc\n",
        "import torch.nn as nn\n",
        "\n",
        "# Pytorch ka Dataset + DataLoader pattern standard hai\n",
        "# Dataset -> how to get one sample\n",
        "# DataLoader → batching, shuffling nd all...\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "# Dataset jo hugging face se lenge...uske liye ye chahiye\n",
        "# 'preinstalled nhi hota: \"pip install datasets\" karna hoga\n",
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "4dldykyHk8c7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 1. CONFIG AND SEED\n",
        "# ======================\n",
        "\n",
        "# I want my experiments to be reproducible.\n",
        "# Using a fixed random seed means shuffling, weight init, etc. will be the same every run.\n",
        "SEED = 42  # classic \"magic\" seed, koi bhi fixed int chalega\n",
        "\n",
        "# Python's own RNG ko seed kar raha hoon (things like random.shuffle, etc.)\n",
        "random.seed(SEED)\n",
        "\n",
        "# PyTorch ke RNG ko bhi same seed deta hoon,\n",
        "# taaki weight initialisation + DataLoader ke kuch random parts stable rahein.\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "\n",
        "# Ab mujhe decide karna hai ki training CPU pe hogi ya GPU pe.\n",
        "# If a CUDA GPU is available, use it; otherwise fall back to CPU.\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. DATASET SIZE CONFIG\n",
        "# ======================\n",
        "\n",
        "# AG News ka pura dataset kaafi bada hai; learning/demo ke liye mujhe\n",
        "# itna bada dataset nahi chahiye, warna training slow ho jayegi.\n",
        "# So I'll cap the number of training and test samples.\n",
        "MAX_TRAIN_SAMPLES = 5000   # small but non-toy size → model kuch sikh lega\n",
        "MAX_TEST_SAMPLES = 2000    # enough to get a reasonable estimate of accuracy\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. FASTTEXT HYPERPARAMETERS\n",
        "# ======================\n",
        "\n",
        "# FastText ka core idea: character n-grams.\n",
        "# Mujhe n ki range choose karni hai. 3–6 typical hai:\n",
        "#  - 3-grams capture small patterns (pre/sufixes)\n",
        "#  - 6-grams thoda longer substrings ko capture karte hain.\n",
        "MIN_N = 3\n",
        "MAX_N = 6\n",
        "\n",
        "# Har unique n-gram ko dictionary me store nahi karna (bohot bada ho sakta hai),\n",
        "# isliye main hashing trick use kar raha hoon.\n",
        "# BUCKET_SIZE = kitne possible hashed IDs (0 .. BUCKET_SIZE-1).\n",
        "# Zyada bucket size → kam collisions, but zyada memory.\n",
        "BUCKET_SIZE = 200_000  # 200k n-gram \"slots\" should be enough for demo\n",
        "\n",
        "\n",
        "# Kitne-dimensional embedding chahiye har n-gram ke liye?\n",
        "# 100 is a nice middle ground: not too small, not too large.\n",
        "EMBED_DIM = 100\n",
        "\n",
        "# ======================\n",
        "# 4. TRAINING HYPERPARAMETERS\n",
        "# ======================\n",
        "\n",
        "# Batch size: bada rakho to training stable hoti hai but memory usage badhta hai.\n",
        "# 64 is a safe default for most GPUs/CPUs for this model.\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Epochs: training passes over the entire dataset.\n",
        "# Demo ke liye 5 epochs rakhta hoon; agar zyada accuracy chahiye,\n",
        "# baad me isse increase kar sakte hain.\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Learning rate for SGD. FastText-style models usually kaafi simple hote hain\n",
        "# (embedding + linear), toh relatively high LR (0.1) often works.\n",
        "# Agar training unstable lagti hai (loss explode ho), isse kam (0.05 ya 0.01) kar sakte hain.\n",
        "LR = 0.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kp51IKNlXqk",
        "outputId": "8cef9159-0467-4297-da5e-1c3d77597326"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NgramEncoder:\n",
        "    \"\"\"\n",
        "    Converts raw text into a 1D tensor of hashed n-gram IDs.\n",
        "    FastText-style: char n-grams from <word> boundaries + whole-word n-gram.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_n=3, max_n=6, bucket_size=200_000):\n",
        "        # Yahan main yeh decide kar raha hoon ki shortest aur longest\n",
        "        # character n-gram kitne length ka hoga.\n",
        "        # Typical FastText ke liye 3–6 ek sensible default hai.\n",
        "        self.min_n = min_n\n",
        "        self.max_n = max_n\n",
        "\n",
        "        # Hashing trick use kar raha hoon instead of explicit vocab dict.\n",
        "        # bucket_size = kitne possible \"slots\" hain jahan n-grams jaa sakte hain.\n",
        "        # int in [0, bucket_size).\n",
        "        self.bucket_size = bucket_size\n",
        "\n",
        "    def _basic_tokenize(self, text):\n",
        "        # Pehle mujhe crude level pe text clean karna hai:\n",
        "        # - Lowercase taaki \"Apple\" aur \"apple\" alag na treat hon.\n",
        "        text = text.lower()\n",
        "\n",
        "        # - Non-alphanumeric characters (punctuation etc.) ko spaces se replace karna,\n",
        "        #   taaki splitting simple ho jaye.\n",
        "        #   Regex \"[^a-z0-9\\s]\" ka matlab: a-z, 0-9, ya whitespace ke alawa sab kuch.\n",
        "        text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "\n",
        "        # Ab simple whitespace split se tokens nikal leta hoon.\n",
        "        tokens = text.split()\n",
        "\n",
        "        # Kuch cases me extra spaces se empty strings aa sakte hain,\n",
        "        # isliye unhe filter out kar deta hoon.\n",
        "        return [t for t in tokens if t]\n",
        "\n",
        "    def _char_ngrams(self, word):\n",
        "        \"\"\"\n",
        "        Create character n-grams with < > boundaries, plus a whole-word token.\n",
        "        Example: word=\"where\" -> \"<where>\"\n",
        "        \"\"\"\n",
        "        # FastText ka trick: word ke start aur end pe boundary markers add karte hain.\n",
        "        # Yahan \"<word>\" ki tarah.\n",
        "        # Isse prefix/suffix properly capture hote hain (e.g., <un, ing> etc.).\n",
        "        w = f\"<{word}>\"\n",
        "        ngrams = []\n",
        "\n",
        "        # Ab main chosen n range (min_n .. max_n) ke liye\n",
        "        # sliding window se character n-grams banaunga.\n",
        "        for n in range(self.min_n, self.max_n + 1):\n",
        "            # Agar word ki length hi n se chhoti hai, to is length ke n-grams\n",
        "            # exist nahi karenge -> continue.\n",
        "            if len(w) < n:\n",
        "                continue\n",
        "\n",
        "            # Standard sliding window:\n",
        "            # w[i : i + n] gives substring of length n starting at i.\n",
        "            for i in range(len(w) - n + 1):\n",
        "                ngrams.append(w[i : i + n])\n",
        "\n",
        "        # FastText typically ek \"full word\" representation bhi rakhta hai.\n",
        "        # Yahan main usko ek special pattern se mark kar raha hoon: \"#<word>#\"\n",
        "        # taaki char n-grams se thoda distinguish ho sake (pure word embedding effect).\n",
        "        ngrams.append(f\"#{w}#\")\n",
        "        return ngrams\n",
        "\n",
        "    def _hash_ngram(self, ngram):\n",
        "        \"\"\"\n",
        "        Hash n-gram string to an integer in [0, bucket_size).\n",
        "        We use md5 for deterministic hashing.\n",
        "        \"\"\"\n",
        "        # Mujhe deterministic mapping chahiye from string -> integer.\n",
        "        # md5 ka hexdigest stable hota hai (same input -> same output).\n",
        "        h = hashlib.md5(ngram.encode(\"utf-8\")).hexdigest()\n",
        "\n",
        "        # Hex string ko base-16 integer me convert kar raha hoon.\n",
        "        # Phir modulo bucket_size se compress kar ke\n",
        "        # valid index range [0, bucket_size) me le aata hoon.\n",
        "        return int(h, 16) % self.bucket_size\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"\n",
        "        text -> LongTensor of hashed n-gram IDs for the whole sentence/document.\n",
        "        \"\"\"\n",
        "        # Step 1: raw text ko tokens me convert karo (basic clean + split).\n",
        "        tokens = self._basic_tokenize(text)\n",
        "\n",
        "        # Yahan IDs store karne ke liye ek normal Python list rakhta hoon,\n",
        "        # baad me ise torch.tensor me convert karunga.\n",
        "        ids = []\n",
        "\n",
        "        # Har token ke liye:\n",
        "        # - uske char n-grams banao\n",
        "        # - har n-gram ko hash karke ID me convert karo\n",
        "        # - sab IDs ek hi list me daal do\n",
        "        for tok in tokens:\n",
        "            for ng in self._char_ngrams(tok):\n",
        "                ids.append(self._hash_ngram(ng))\n",
        "\n",
        "        # Edge case: agar cleaning ke baad text khaali ho gaya\n",
        "        # (e.g., sirf punctuation tha) to ids list empty hogi.\n",
        "        # EmbeddingBag ko kam se kam ek index chahiye, warna crash ho sakta hai.\n",
        "        # Isliye yahan ek dummy id (0) push kar raha hoon.\n",
        "        if not ids:\n",
        "            ids.append(0)\n",
        "\n",
        "        # Finally, PyTorch model ke liye LongTensor banana zaruri hai\n",
        "        # (embedding layers indexes ko Long type chahiye).\n",
        "        return torch.tensor(ids, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "fF6jTIXDzRS_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastTextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wrap HF dataset into a PyTorch Dataset:\n",
        "    returns (ngram_ids_tensor, label_int).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, encoder: NgramEncoder):\n",
        "        # Sabse pehle sanity check: texts aur labels ki length same honi chahiye.\n",
        "        # Agar mismatch ho, matlab data corrupt/config galat hai → turant assert fail karwa do.\n",
        "        assert len(texts) == len(labels)\n",
        "\n",
        "        # Raw texts (list of strings ya HF objects) ko store kar leta hoon.\n",
        "        # Main unko yahin encode nahi kar raha, kyunki encoding thoda heavy hai.\n",
        "        # Instead, __getitem__ pe on-the-fly encode karunga (lazy encoding).\n",
        "        self.texts = texts\n",
        "\n",
        "        # Labels ko bhi ek list ke form me store kar raha hoon (integers hone chahiye).\n",
        "        self.labels = labels\n",
        "\n",
        "        # Encoder object (NgramEncoder) ko store karna zaroori hai,\n",
        "        # taki har sample ke liye text -> n-gram IDs convert kar saku.\n",
        "        self.encoder = encoder\n",
        "\n",
        "    def __len__(self):\n",
        "        # DataLoader ko pata hona chahiye dataset me kitne samples hain.\n",
        "        # Simple answer: jitne texts hain utne hi samples.\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # DataLoader jab koi index maangta hai (0..len-1),\n",
        "        # to yahan se ek single sample return hoga.\n",
        "\n",
        "        # Kabhi-kabhi HF dataset elements string ke alawa bhi object ho sakte hain,\n",
        "        # so main defensive programming kar raha hoon: ensure it's a string.\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        # Labels ideally already int hone chahiye, but\n",
        "        # safe side: explicitly int() cast kar deta hoon.\n",
        "        label = int(self.labels[idx])\n",
        "\n",
        "        # Ab main encoder ko call karta hoon:\n",
        "        # raw sentence -> 1D LongTensor of hashed n-gram IDs.\n",
        "        # Ye hi humara \"input representation\" hai model ke liye.\n",
        "        ngram_ids = self.encoder.encode(text)  # 1D LongTensor\n",
        "\n",
        "        # Dataset ka contract: __getitem__ returns a single training example.\n",
        "        # Yahan main (ngram_ids_tensor, label_int) return kar raha hoon.\n",
        "        return ngram_ids, label\n"
      ],
      "metadata": {
        "id": "2JNq2TYJrqQj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    batch: list of (ngram_ids_tensor, label_int)\n",
        "    We need:\n",
        "      - text: all n-gram ids concatenated into one long tensor\n",
        "      - offsets: start index of each example inside text\n",
        "      - labels: tensor of labels\n",
        "    Suitable for nn.EmbeddingBag.\n",
        "    \"\"\"\n",
        "\n",
        "    # DataLoader ne batch bana ke diya hai ek list ke form me:\n",
        "    #   [ (ngram_ids_0, label_0),\n",
        "    #     (ngram_ids_1, label_1),\n",
        "    #     ... ]\n",
        "    #\n",
        "    # Mujhe inn ko alag-alag lists me todna hai:\n",
        "    #   - ngram_tensors: [tensor0, tensor1, ...]\n",
        "    #   - labels: [l0, l1, ...]\n",
        "    #\n",
        "    # zip(*batch) exactly ye kaam karta hai -> transpose jaisa behavior.\n",
        "    ngram_tensors, labels = zip(*batch)  # list of tensors, list of ints\n",
        "\n",
        "    # ==========================\n",
        "    # Offsets banana (EmbeddingBag style)\n",
        "    # ==========================\n",
        "    # EmbeddingBag expect karta hai:\n",
        "    #   text: ek single 1D tensor jisme saare examples ke n-gram IDs concat hon\n",
        "    #   offsets: 1D tensor jiska har element batata hai\n",
        "    #            \"ye example kahan se start ho raha hai 'text' ke andar?\"\n",
        "    #\n",
        "    # Example:\n",
        "    #   example 0 ids: [5, 10, 11]  (len=3)\n",
        "    #   example 1 ids: [7, 8]       (len=2)\n",
        "    #\n",
        "    #   text    = [5, 10, 11, 7, 8]\n",
        "    #   offsets = [0, 3]\n",
        "    #\n",
        "    #   EmbeddingBag ko pata chal jaata hai:\n",
        "    #     - sample 0 -> text[0:3]\n",
        "    #     - sample 1 -> text[3:5]\n",
        "\n",
        "    # Main offsets list [0] se start karta hoon (first sample always index 0 se start).\n",
        "    offsets = [0]\n",
        "\n",
        "    # Ab har tensor ko (last wale ko chhod ke) dekhunga:\n",
        "    # next offset = previous offset + length of current tensor\n",
        "    for t in ngram_tensors[:-1]:\n",
        "        offsets.append(offsets[-1] + len(t))\n",
        "\n",
        "    # Python list ko PyTorch LongTensor me convert karna zaroori hai,\n",
        "    # kyunki EmbeddingBag ko Long dtype chahiye.\n",
        "    offsets = torch.tensor(offsets, dtype=torch.long)\n",
        "\n",
        "    # ==========================\n",
        "    # text banana (concat all IDs)\n",
        "    # ==========================\n",
        "    # Ab mujhe saare individual n-gram id tensors ko ek single 1D vector me\n",
        "    # jodna hai. torch.cat exactly yahi karta hai.\n",
        "    #\n",
        "    # Note: yeh sab tensors already 1D hain (thanks to encoder),\n",
        "    # isliye dim=0 default hi sahi hai.\n",
        "    text = torch.cat(ngram_tensors)\n",
        "\n",
        "    # Labels ko bhi list se tensor me convert kar raha hoon.\n",
        "    # CrossEntropyLoss etc. ko LongTensor labels chahiye hote hain.\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    # Final return:\n",
        "    #   - text: [total_ngrams_in_batch]\n",
        "    #   - offsets: [batch_size]\n",
        "    #   - labels: [batch_size]\n",
        "    return text, offsets, labels\n"
      ],
      "metadata": {
        "id": "BwjY_5Fu43oS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastTextClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    FastText-style classifier:\n",
        "      - EmbeddingBag over n-gram ids (mean pooling)\n",
        "      - Linear layer to num_classes\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
        "        # Sabse pehle main soch raha hoon: yeh PyTorch module hai,\n",
        "        # toh nn.Module ko properly init karna hoga.\n",
        "        super().__init__()\n",
        "\n",
        "        # FastText ka core idea: ek simple bag-of-ngrams embedding + linear classifier.\n",
        "        #\n",
        "        # Yahan main nn.EmbeddingBag use kar raha hoon instead of plain nn.Embedding:\n",
        "        #  - EmbeddingBag directly multiple indices ka mean/sum nikal deta hai\n",
        "        #    (bag-of-words style), bina manually average karne ke.\n",
        "        #  - \"mode='mean'\" ka matlab: har sentence ke n-gram embeddings ka average lo.\n",
        "        #\n",
        "        # vocab_size  -> hashing bucket size (kitne possible n-gram IDs)\n",
        "        # embed_dim   -> har n-gram ka vector size\n",
        "        self.embedding = nn.EmbeddingBag(\n",
        "            vocab_size, embed_dim, mode=\"mean\"\n",
        "        )\n",
        "\n",
        "        # Ab mujhe sentence embedding (embed_dim) ko class logits me map karna hai.\n",
        "        # Sabse simple classifier: ek linear layer.\n",
        "        # Input: [batch_size, embed_dim]\n",
        "        # Output: [batch_size, num_classes]\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Custom weight initialization karna hai, taaki training stable start ho.\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize embeddings and linear layer\n",
        "        #\n",
        "        # Yahan main ek chhota sa init range decide kar raha hoon:\n",
        "        # 0.5 / EMBED_DIM -> jaise-jaise dimension badhta hai,\n",
        "        # range chhoti ho jati hai, taaki values zyada spread out na ho.\n",
        "        # (Note: yeh global EMBED_DIM use kar raha hoon;\n",
        "        #  alternatively, embed_dim argument se bhi le sakta tha.)\n",
        "        init_range = 0.5 / EMBED_DIM\n",
        "\n",
        "        # Embedding weights ko uniform distribution se init kar raha hoon\n",
        "        # [-init_range, init_range] ke beech.\n",
        "        self.embedding.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "        # Linear layer ke weights ko bhi same range me init kar raha hoon.\n",
        "        self.fc.weight.data.uniform_(-init_range, init_range)\n",
        "\n",
        "        # Bias ka best default: zero se start karo.\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        # forward pass me mujhe 2 cheeze milti hain:\n",
        "        #   text:    [N_total_ngrams_in_batch]\n",
        "        #   offsets: [batch_size]\n",
        "        #\n",
        "        # Ye exactly woh format hai jo collate_fn ne banaya tha\n",
        "        # EmbeddingBag ke liye.\n",
        "\n",
        "        # EmbeddingBag yahan ye karega:\n",
        "        #   - offsets ke hisaab se text ko sentences me split karega\n",
        "        #   - har sentence ke indices ke embeddings ka mean lega\n",
        "        #\n",
        "        # Result: embedded shape = [batch_size, embed_dim]\n",
        "        embedded = self.embedding(text, offsets)  # [batch_size, embed_dim]\n",
        "\n",
        "        # Ab simple linear classifier:\n",
        "        #   embedded -> logits (pre-softmax scores)\n",
        "        # logits shape: [batch_size, num_classes]\n",
        "        logits = self.fc(embedded)                # [batch_size, num_classes]\n",
        "\n",
        "        # CrossEntropyLoss automatically logits + integer labels se\n",
        "        # softmax + negative log-likelihood handle karega,\n",
        "        # isliye yahan sirf logits return karna kaafi hai.\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "yvj1rryV-hIj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pehle main user ko thoda feedback dena chahta hoon ki ab dataset load ho raha hai,\n",
        "# taaki console me silent na lage.\n",
        "print(\"Loading AG News dataset from Hugging Face...\")\n",
        "\n",
        "# Hugging Face datasets library ka built-in text classification dataset use kar raha hoon:\n",
        "# \"ag_news\" -> 4 news categories (World, Sports, Business, Sci/Tech).\n",
        "# Isme already 'train' aur 'test' splits defined hain.\n",
        "raw_dataset = load_dataset(\"ag_news\")\n",
        "\n",
        "\n",
        "# Ab mujhe class labels ki info chahiye.\n",
        "# HF dataset me label feature ke andar names stored hote hain.\n",
        "label_names = raw_dataset[\"train\"].features[\"label\"].names\n",
        "\n",
        "# Kitni classes hain? -> simply len(label_names)\n",
        "num_classes = len(label_names)\n",
        "\n",
        "print(\"Classes:\", label_names)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Limit dataset size for learning/demo\n",
        "# ==========================\n",
        "# Pura AG News training set ~120k samples hai.\n",
        "# Learning purpose ke liye mujhe itna bada dataset nahi chahiye,\n",
        "# warna training slow ho jayegi, especially CPU pe.\n",
        "#\n",
        "# Isliye upar define kiya hua MAX_TRAIN_SAMPLES / MAX_TEST_SAMPLES use karke\n",
        "# subset le raha hoon.\n",
        "\n",
        "# Training texts aur labels (subset)\n",
        "train_texts = raw_dataset[\"train\"][\"text\"][:MAX_TRAIN_SAMPLES]\n",
        "train_labels = raw_dataset[\"train\"][\"label\"][:MAX_TRAIN_SAMPLES]\n",
        "\n",
        "# Test texts aur labels (subset)\n",
        "test_texts = raw_dataset[\"test\"][\"text\"][:MAX_TEST_SAMPLES]\n",
        "test_labels = raw_dataset[\"test\"][\"label\"][:MAX_TEST_SAMPLES]\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# Encoder + Dataset objects\n",
        "# ==========================\n",
        "\n",
        "# Ab mujhe NgramEncoder ka ek instance chahiye,\n",
        "# jo raw text ko hashed n-gram IDs me convert kare.\n",
        "# MIN_N, MAX_N, BUCKET_SIZE global config se aa rahe hain.\n",
        "encoder = NgramEncoder(min_n=MIN_N, max_n=MAX_N, bucket_size=BUCKET_SIZE)\n",
        "\n",
        "# Training ke liye FastTextDataset banata hoon\n",
        "# jo HF lists ko PyTorch Dataset interface me wrap karega.\n",
        "train_dataset = FastTextDataset(train_texts, train_labels, encoder)\n",
        "\n",
        "# Similarly, test side ke liye bhi ek Dataset\n",
        "test_dataset = FastTextDataset(test_texts, test_labels, encoder)\n",
        "\n",
        "\n",
        "# ==========================\n",
        "# DataLoader (batching, shuffling)\n",
        "# ==========================\n",
        "\n",
        "# Training DataLoader:\n",
        "# - batch_size = BATCH_SIZE (e.g., 64)\n",
        "# - shuffle=True: har epoch me data ko randomize karna, taaki training better generalize kare.\n",
        "# - collate_fn=collate_batch: humara custom collate function jo\n",
        "#   n-gram tensors ko concat + offsets bana deta hai (EmbeddingBag format).\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        ")\n",
        "\n",
        "# Test DataLoader:\n",
        "# - shuffle=False: evaluation ke liye order matter nahi karta,\n",
        "#   but deterministic rakhna sahi habit hai.\n",
        "# - baaki sab train jaisa, sirf dataset change.\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhAaTjd_FEbb",
        "outputId": "2749076e-95d7-47c9-a8d9-74165fa0a522"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading AG News dataset from Hugging Face...\n",
            "Classes: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 6. Train & Evaluate\n",
        "# ======================\n",
        "\n",
        "# Ab tak mere paas:\n",
        "#  - encoder ready\n",
        "#  - Dataset + DataLoader ready\n",
        "#  - config ready\n",
        "# Ab mujhe actual FastText classifier ka instance banana hai.\n",
        "\n",
        "model = FastTextClassifier(\n",
        "    vocab_size=BUCKET_SIZE,   # hashing bucket size (kitne possible n-gram IDs)\n",
        "    embed_dim=EMBED_DIM,      # har n-gram ka vector size\n",
        "    num_classes=num_classes,  # AG News ke 4 classes\n",
        ").to(DEVICE)                  # model ko CPU/GPU jahan bhi available ho, wahan bhej do\n",
        "\n",
        "# Classification problem hai, logits + integer labels:\n",
        "# CrossEntropyLoss is the standard choice (softmax + NLL combined).\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer choose karna hai.\n",
        "# FastText jaisa simple linear+embedding model ke liye SGD kaafi acha kaam karta hai.\n",
        "# LR pehle config me set kiya tha (LR = 0.1).\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    # Evaluation mode:\n",
        "    #  - dropout, batchnorm jaise layers alag behave karte hain, isliye model.eval() zaroori\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Evaluation me gradients ki zarurat nahi hoti,\n",
        "    # to torch.no_grad() use karke memory + compute dono bachate hain.\n",
        "    with torch.no_grad():\n",
        "        for text, offsets, labels in dataloader:\n",
        "            # Batch ko correct device (CPU/GPU) pe move karo.\n",
        "            text = text.to(DEVICE)\n",
        "            offsets = offsets.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Forward pass se logits milenge.\n",
        "            logits = model(text, offsets)\n",
        "\n",
        "            # Loss compute karo (per batch).\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # total_loss me \"sum of losses weighted by batch size\" store kar raha hoon,\n",
        "            # taaki baad me average loss nikal saku:\n",
        "            # avg_loss = total_loss / total_examples\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            # Predictions: logits ka argmax class dimension (dim=1) pe.\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            # Kitne correct predictions hue is batch me?\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Total examples count bhi maintain karna hoga.\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "    # Loop ke baad: average loss aur accuracy nikal lo.\n",
        "    avg_loss = total_loss / total_examples\n",
        "    accuracy = 100.0 * total_correct / total_examples\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "print(\"Starting training...\")\n",
        "\n",
        "# Epoch loop:\n",
        "# Har epoch = poora training dataset ek baar dekhna.\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Training mode ON (dropout, batchnorm etc. training behavior).\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Train DataLoader se batch-by-batch data aayega.\n",
        "    for text, offsets, labels in train_loader:\n",
        "        # Batch ko correct device pe move karo.\n",
        "        text = text.to(DEVICE)\n",
        "        offsets = offsets.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Har step pe gradients ko reset karna zaroori hai,\n",
        "        # warna PyTorch previous gradients accumulate karta hai.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: logits compute karo.\n",
        "        logits = model(text, offsets)\n",
        "\n",
        "        # Loss compute karo current batch ka.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass: dLoss/dParams calculate karo.\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step: parameters ko update karo gradients ke according.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging ke liye training loss + accuracy track karta hoon.\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        # Predictions for accuracy:\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "    # Ek epoch khatam hone ke baad:\n",
        "    # Average training loss and accuracy nikalte hain.\n",
        "    train_loss = total_loss / total_examples\n",
        "    train_acc = 100.0 * total_correct / total_examples\n",
        "\n",
        "    # Validation / test metrics evaluate() function se:\n",
        "    val_loss, val_acc = evaluate(model, test_loader)\n",
        "\n",
        "    # Console pe progress print kar raha hoon,\n",
        "    # taaki pata chale training improve ho rahi hai ya nahi.\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "        f\"Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "print(\"Training complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUfOp1u5GO8O",
        "outputId": "21d00071-e343-4fde-e091-adae7db20a5d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5 | Train Loss: 1.3803, Train Acc: 29.84% | Test Loss: 1.3955, Test Acc: 25.70%\n",
            "Epoch 2/5 | Train Loss: 1.3781, Train Acc: 29.94% | Test Loss: 1.3973, Test Acc: 25.70%\n",
            "Epoch 3/5 | Train Loss: 1.3779, Train Acc: 29.94% | Test Loss: 1.3964, Test Acc: 25.70%\n",
            "Epoch 4/5 | Train Loss: 1.3781, Train Acc: 29.94% | Test Loss: 1.3957, Test Acc: 25.70%\n",
            "Epoch 5/5 | Train Loss: 1.3780, Train Acc: 29.94% | Test Loss: 1.3964, Test Acc: 25.70%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader):\n",
        "    # Evaluation mode:\n",
        "    #  - dropout, batchnorm jaise layers alag behave karte hain, isliye model.eval() zaroori\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Evaluation me gradients ki zarurat nahi hoti,\n",
        "    # to torch.no_grad() use karke memory + compute dono bachate hain.\n",
        "    with torch.no_grad():\n",
        "        for text, offsets, labels in dataloader:\n",
        "            # Batch ko correct device (CPU/GPU) pe move karo.\n",
        "            text = text.to(DEVICE)\n",
        "            offsets = offsets.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Forward pass se logits milenge.\n",
        "            logits = model(text, offsets)\n",
        "\n",
        "            # Loss compute karo (per batch).\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            # total_loss me \"sum of losses weighted by batch size\" store kar raha hoon,\n",
        "            # taaki baad me average loss nikal saku:\n",
        "            # avg_loss = total_loss / total_examples\n",
        "            total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "            # Predictions: logits ka argmax class dimension (dim=1) pe.\n",
        "            preds = logits.argmax(dim=1)\n",
        "\n",
        "            # Kitne correct predictions hue is batch me?\n",
        "            total_correct += (preds == labels).sum().item()\n",
        "\n",
        "            # Total examples count bhi maintain karna hoga.\n",
        "            total_examples += labels.size(0)\n",
        "\n",
        "    # Loop ke baad: average loss aur accuracy nikal lo.\n",
        "    avg_loss = total_loss / total_examples\n",
        "    accuracy = 100.0 * total_correct / total_examples\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "J_QyRQSOHVfL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training...\")\n",
        "\n",
        "# Epoch loop:\n",
        "# Har epoch = poora training dataset ek baar dekhna.\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # Training mode ON (dropout, batchnorm etc. training behavior).\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    # Train DataLoader se batch-by-batch data aayega.\n",
        "    for text, offsets, labels in train_loader:\n",
        "        # Batch ko correct device pe move karo.\n",
        "        text = text.to(DEVICE)\n",
        "        offsets = offsets.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Har step pe gradients ko reset karna zaroori hai,\n",
        "        # warna PyTorch previous gradients accumulate karta hai.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass: logits compute karo.\n",
        "        logits = model(text, offsets)\n",
        "\n",
        "        # Loss compute karo current batch ka.\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward pass: dLoss/dParams calculate karo.\n",
        "        loss.backward()\n",
        "\n",
        "        # Optimizer step: parameters ko update karo gradients ke according.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging ke liye training loss + accuracy track karta hoon.\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        # Predictions for accuracy:\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += (preds == labels).sum().item()\n",
        "        total_examples += labels.size(0)\n",
        "\n",
        "    # Ek epoch khatam hone ke baad:\n",
        "    # Average training loss and accuracy nikalte hain.\n",
        "    train_loss = total_loss / total_examples\n",
        "    train_acc = 100.0 * total_correct / total_examples\n",
        "\n",
        "    # Validation / test metrics evaluate() function se:\n",
        "    val_loss, val_acc = evaluate(model, test_loader)\n",
        "\n",
        "    # Console pe progress print kar raha hoon,\n",
        "    # taaki pata chale training improve ho rahi hai ya nahi.\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{NUM_EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "        f\"Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.2f}%\"\n",
        "    )\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckRPeyAeH_B9",
        "outputId": "37b5fe83-681e-4fbf-b002-4900b1de95c6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Epoch 1/5 | Train Loss: 1.3781, Train Acc: 29.94% | Test Loss: 1.3947, Test Acc: 25.70%\n",
            "Epoch 2/5 | Train Loss: 1.3781, Train Acc: 29.94% | Test Loss: 1.3941, Test Acc: 25.70%\n",
            "Epoch 3/5 | Train Loss: 1.3783, Train Acc: 29.94% | Test Loss: 1.3970, Test Acc: 25.70%\n",
            "Epoch 4/5 | Train Loss: 1.3782, Train Acc: 29.94% | Test Loss: 1.3943, Test Acc: 25.70%\n",
            "Epoch 5/5 | Train Loss: 1.3782, Train Acc: 29.94% | Test Loss: 1.3945, Test Acc: 25.70%\n",
            "Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================\n",
        "# 7. Inference Demo\n",
        "# ======================\n",
        "\n",
        "def predict(text: str):\n",
        "    # Prediction time pe mujhe ensure karna hai ki model evaluation mode me ho:\n",
        "    #  - dropout, batchnorm jaise layers (agar hote) inference behavior use karein.\n",
        "    model.eval()\n",
        "\n",
        "    # Inference ke liye gradients ki zarurat nahi hai,\n",
        "    # to torch.no_grad() use karke compute + memory dono bachata hoon.\n",
        "    with torch.no_grad():\n",
        "        # Step 1: raw input text ko wahi encoder se pass karna hai\n",
        "        # jo training me use kiya tha, taaki representation consistent rahe.\n",
        "        # encoder.encode() -> 1D LongTensor of n-gram IDs.\n",
        "        ngram_ids = encoder.encode(text).to(DEVICE)\n",
        "\n",
        "        # EmbeddingBag ko do cheeze chahiye: text + offsets.\n",
        "        # Yahan batch size = 1 hai, to offsets hamesha [0] hoga\n",
        "        # (single example ke n-grams text[0:] se start ho rahe hain).\n",
        "        offsets = torch.tensor([0], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "        # Forward pass:\n",
        "        #  - ngram_ids: [N_ngrams]\n",
        "        #  - offsets: [1]\n",
        "        # Output: logits shape [1, num_classes]\n",
        "        logits = model(ngram_ids, offsets)\n",
        "\n",
        "        # Mujhe human-friendly probabilities chahiye,\n",
        "        # isliye softmax laga raha hoon class dimension (dim=1) pe.\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        # Ab highest probability wali class ka index nikalna hai.\n",
        "        # probs.argmax(dim=1) -> tensor([class_id])\n",
        "        # .item() se python int banaya, fir explicit int(...) for safety.\n",
        "        pred_label_id = int(probs.argmax(dim=1).item())\n",
        "\n",
        "        # label_names list me se us class id ka naam le leta hoon\n",
        "        # (e.g., \"World\", \"Sports\", etc.) aur confidence (probability) bhi return karta hoon.\n",
        "        return label_names[pred_label_id], probs[0, pred_label_id].item()\n",
        "\n",
        "\n",
        "# Ab ek chhota sanity check / demo:\n",
        "# Ek sentence leke dekhte hain model kya predict karta hai.\n",
        "example_text = \"Apple introduces new phone model for the Indian market.\"\n",
        "\n",
        "# predict() se label + confidence tuple milega.\n",
        "pred_label, confidence = predict(example_text)\n",
        "\n",
        "# Output thoda readable form me print kar dete hain.\n",
        "print(\"\\nExample text:\", example_text)\n",
        "print(f\"Predicted class: {pred_label} (confidence: {confidence:.3f})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HayA_wb2IV_p",
        "outputId": "d08b5a3d-990a-4cee-d453-7e7d2e3646c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example text: Apple introduces new phone model for the Indian market.\n",
            "Predicted class: Sci/Tech (confidence: 0.288)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Book Implementation"
      ],
      "metadata": {
        "id": "r38CBVauIzJY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "metadata": {
        "id": "0t7llB1WTpv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLRo8lwMTsBA",
        "outputId": "06c982c1-dd9e-45b5-84bd-a59a816970c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "metadata": {
        "id": "pV08Y9HeTr9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_document(doc):\n",
        "  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "  doc = doc.lower()\n",
        "  doc = doc.strip()\n",
        "\n",
        "  tokens = nltk.word_tokenize(doc)\n",
        "\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "  doc = ' '.join(filtered_tokens)\n",
        "\n",
        "  return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)"
      ],
      "metadata": {
        "id": "Bwa-1YSOTr7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cats = ['alt.atheism', 'sci.space']\n",
        "newsgroup_train = fetch_20newsgroups(subset='train',\n",
        "                                     categories=cats,\n",
        "                                     remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "Bd_sXxw5Tr4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of news articles = {}'.format(len(newsgroup_train.data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnny65TgTr1N",
        "outputId": "c42cef6a-60e0-488d-c066-d66b5474b259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of news articles = 1073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm_corpus = normalize_corpus(newsgroup_train.data)\n",
        "norm_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpmzr6KTryl",
        "outputId": "291a3af9-b3f8-46d4-dbaa-bdcc08a270c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['please enlighten omnipotence contradictory definition occur universe governed rules nature thus god break anything god must allowed rules somewhere therefore omnipotence exist contradicts rules nature obviously omnipotent god change rules say definition exactly defined certainly omnipotence seem saying rules nature preexistant somehow define nature actually cause thats mean id like hear thoughts question',\n",
              "       'aprkelvinjplnasagov baalkekelvinjplnasagov sorry think missed bit info transition experiment mean loss data magellan transmit data later btw nasa cut connection magellan looking forward day curious believe something funding goverment rather funding ok thats see guys around jurriaan',\n",
              "       'henry made assumption gets firstest mostest wins ohhh want put fine print says thou shall wonderous rd rather use offtheshelf hardware sorry didnt see copy pournellesque proposals run along lines dollar amount reward simple goal go ahead development ill buy shelf higher cost even russian also assume thered buy us provos camped moon launching assembling little ittybitty payloads leo laser gas gun working bugs assembly integration leo oh hey could get couple canadarms tuned lunar environment wan na teleoperated prospecting im',\n",
              "       ...,\n",
              "       'publish plenty kiddoyou look sig files like strings every yoyos got one',\n",
              "       'okay lets get record straight livermore gas gun project manager dr john hunter works laser group livermore may ask gas guns lasers nothing really gun physically located across road free electron laser building fel building heavily shielded control room thick walls gun firings controlled suspect office works administrative convenience visited hunter beginning feb toured gun time working gas gun rd boeing work things helping save space station gun uses methaneair mixture burned chamber ft long inch id ie looks like pipe chamber holds ton piston propelled several hundred ms chamber side piston hudrogen gas initially room temperature andsome tens atmospheres piston compresses heats hydrogen ahead stainless steel burst diaphragm ruptures around psi barrel gun feet long inch bore mounted right angles chamber ie intersect done future barrel could raised gun fired air without move larger heavier chamber projectile used testing kg cylinder lexan plastic diameter cm long acceleration comes expansion hydrogen gas psi downwards projectile leaves barrel barrel evacuated end sealed sheet plastic film little thicker saran wrap plastic blown small amount residual air trapped barrel ahead projectile gun fired bunker filled sandbags plastic water jugs early testing fragments plastic projectile found higher speeds later testing projectile vaporizes testing bunker livermore test range miles across projectile would go km fired maximum range intent move whole gun vandenberg afb testing complete fire pacific ocean use tracking radar vafb follow projectiles design goal gun throw kg projectile kms half orbital speed far reached kms gun currently repairs last test blew seal damaged hardware think methaneair detonating burning havent chance talk hunter directly people waiting test scramjet components gun firing gun air mach kms since get wind tunnels mach gun cost million develop basically proofofconcept bigger gun capable firing useful sized payloads space would require order kg projectiles deliver order kg useful payload orbit dani eder',\n",
              "       'known quite earth actually pear shaped globularspherical anyone make globe accurate actual shape landmass configurationlonglat lines etc thanks advance billxpressouucp bill vance bothell wa rwingxpressobill'],\n",
              "      dtype='<U32796')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOeMXPK3Trv2",
        "outputId": "be9de606-5347-4e64-c7d1-264c535206f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erZDG26nNieu"
      },
      "outputs": [],
      "source": [
        "from gensim.models.fasttext import FastText"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qfi_YZELTrs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]"
      ],
      "metadata": {
        "id": "k7r3_1KnNmna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "_QFowCTzNmkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "embedding_size = 32\n",
        "context_window = 20\n",
        "min_word_count = 1\n",
        "sample = 1e-3\n",
        "sg = 1\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "ft_model = FastText(tokenized_corpus,\n",
        "                    vector_size=embedding_size,\n",
        "                    window=context_window,\n",
        "                    min_count=min_word_count,\n",
        "                    sg = sg,\n",
        "                    sample=sample,\n",
        "                    epochs=100)\n",
        "\n",
        "end_time = time.time()\n",
        "time_taken = end_time - start_time\n",
        "print(f\"Time taken to train FastText model: {time_taken:.2f} seconds\")"
      ],
      "metadata": {
        "id": "FuCYf7eJNmh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c43c217-42fb-4b46-a487-06cb2eb0894d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to train FastText model: 169.64 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minutes, seconds = divmod(time_taken, 60)\n",
        "print(f\"Time taken to train FastText model: {int(minutes)} min {seconds:.2f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBCEGo9kV3TF",
        "outputId": "ae2823c2-0d5b-4fa4-a816-a0ef020f3e96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to train FastText model: 2 min 49.64 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Unique numbers of words in the model={ft_model.wv.vectors.shape[0]}\")"
      ],
      "metadata": {
        "id": "cEWI7dA3Nmcy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3971f1bd-3929-4fb8-e60f-2c4f3ed56c44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique numbers of words in the model=19421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv['sun']"
      ],
      "metadata": {
        "id": "O75P0NpoNmZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8dd638b-1729-4077-a4fb-2aaad0393b2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.68865156,  0.16804571, -0.9665691 , -0.3416829 ,  0.78023183,\n",
              "        0.25930217,  0.3499796 , -0.48844534, -0.5560808 , -0.5228781 ,\n",
              "       -0.50807303, -0.55770886, -0.3148962 , -0.9736987 , -0.31482577,\n",
              "       -0.22821371,  0.06310669,  0.5029337 ,  0.2532907 , -0.4708344 ,\n",
              "        0.29784685, -0.25402132,  0.19896385, -0.02769833,  0.23079884,\n",
              "       -0.32078335, -1.1128368 , -0.45634848, -0.19624002, -0.15969302,\n",
              "       -0.03623481, -0.34593585], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv['sunny']"
      ],
      "metadata": {
        "id": "hJAyI6iONmXT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2942067f-6872-43b4-ddb3-025bf9be4ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.7969927 ,  1.3445771 , -0.8033236 , -0.31200108,  0.69085395,\n",
              "        0.39706072,  1.8380454 , -0.5289925 , -1.0682884 , -1.4317443 ,\n",
              "       -0.8753653 ,  0.34827802, -1.9453781 , -0.0436891 , -0.33408877,\n",
              "        0.39011073,  0.0323919 ,  1.2287263 ,  0.13863759, -0.64394426,\n",
              "        0.41518155,  0.19215867,  0.3469433 ,  0.41325116, -0.41990545,\n",
              "        0.8155734 , -0.7166068 , -0.11166925,  0.44339675, -0.59900945,\n",
              "        0.32291928, -1.049846  ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv.most_similar(positive=['god'])"
      ],
      "metadata": {
        "id": "JybTdlveNmUy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5d4f86-6c79-48f6-a7d6-75dcb458b2c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('existence', 0.863061249256134),\n",
              " ('interestingly', 0.8570623993873596),\n",
              " ('ontology', 0.8477120995521545),\n",
              " ('eternal', 0.8461553454399109),\n",
              " ('nonexistence', 0.8411461114883423),\n",
              " ('undying', 0.8393584489822388),\n",
              " ('believing', 0.8391947746276855),\n",
              " ('denials', 0.8355113863945007),\n",
              " ('exists', 0.8312482237815857),\n",
              " ('trivially', 0.8309420943260193)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model.wv.most_similar(positive=['sunny'])"
      ],
      "metadata": {
        "id": "C1uGocQmNmR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0068267-6c30-4445-b5d5-ddd9e4991472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wilderness', 0.8077141642570496),\n",
              " ('much', 0.8049019575119019),\n",
              " ('bind', 0.7919706106185913),\n",
              " ('would', 0.7868427634239197),\n",
              " ('matter', 0.7867603302001953),\n",
              " ('renounce', 0.7801792025566101),\n",
              " ('concrete', 0.7763912677764893),\n",
              " ('going', 0.7736693620681763),\n",
              " ('mas', 0.773422122001648),\n",
              " ('orifices', 0.7714561820030212)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_Ws_2S_NmOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}