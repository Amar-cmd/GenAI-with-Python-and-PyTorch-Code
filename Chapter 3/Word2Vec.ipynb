{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"L4","mount_file_id":"1WU4ss90e2kpERjBbfCrc4nI2VaHyFPlZ","authorship_tag":"ABX9TyO1wrzSfFvKYA9f0/i6w5W8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"22a9e09887d941a1b215f92ebc0617e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb195e7828e8461b88355c37424d8742","IPY_MODEL_8835be5ea9f348788f506437ecfa1572","IPY_MODEL_c0514ee7082d4544ad67acd0ec0c2430"],"layout":"IPY_MODEL_522ba1b89e44462f9b5f24405c43bc4f"}},"fb195e7828e8461b88355c37424d8742":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83534db02d574382a0e95ce207f0a112","placeholder":"​","style":"IPY_MODEL_f3ae4abb4f814b90a7cdb090c1d6318c","value":"README.md: "}},"8835be5ea9f348788f506437ecfa1572":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2ed3b83276c3444da1c7f42b086a043c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_213f220d1b1c4381b11f18cd7793da62","value":1}},"c0514ee7082d4544ad67acd0ec0c2430":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ac80fe9491f4f58af910ad5a9bb69d9","placeholder":"​","style":"IPY_MODEL_ff808f9245d740c68dd6b5bddb4c55d6","value":" 10.5k/? [00:00&lt;00:00, 1.26MB/s]"}},"522ba1b89e44462f9b5f24405c43bc4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83534db02d574382a0e95ce207f0a112":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3ae4abb4f814b90a7cdb090c1d6318c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2ed3b83276c3444da1c7f42b086a043c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"213f220d1b1c4381b11f18cd7793da62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5ac80fe9491f4f58af910ad5a9bb69d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff808f9245d740c68dd6b5bddb4c55d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2a6f3eb6f884a7c80b346e6b8bf26bd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4519142bad8c4438a73da0e76ff7eaa5","IPY_MODEL_f619d79994ee489284b3e32136fd8cf7","IPY_MODEL_0655089100794e2c93fcad84b4e5d3f3"],"layout":"IPY_MODEL_c16bc80d1fc8492cb71f478d522fe632"}},"4519142bad8c4438a73da0e76ff7eaa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55402d823cea42ee8520ec9a30a9afca","placeholder":"​","style":"IPY_MODEL_aed6482c8ee64189a81a74c67511d19b","value":"wikitext-2-raw-v1/test-00000-of-00001.pa(…): 100%"}},"f619d79994ee489284b3e32136fd8cf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_065a95c5cd784290897133663ecf3796","max":732610,"min":0,"orientation":"horizontal","style":"IPY_MODEL_203c03d58abf462b95d1ad9911903839","value":732610}},"0655089100794e2c93fcad84b4e5d3f3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4b59c59fdc4e6ba04f0029e07e11f6","placeholder":"​","style":"IPY_MODEL_a0fe57defced4c72b9c22c7bef75d747","value":" 733k/733k [00:00&lt;00:00, 48.6kB/s]"}},"c16bc80d1fc8492cb71f478d522fe632":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55402d823cea42ee8520ec9a30a9afca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aed6482c8ee64189a81a74c67511d19b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"065a95c5cd784290897133663ecf3796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"203c03d58abf462b95d1ad9911903839":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5a4b59c59fdc4e6ba04f0029e07e11f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0fe57defced4c72b9c22c7bef75d747":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e08d0662d69f4efc90b7715ca862cc70":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2dd5e840ee7040629ed67684ea5568f7","IPY_MODEL_03e3e850ae294658a74b5fea19171910","IPY_MODEL_d48d9fed58b042fd8f7f8b1db558d6cd"],"layout":"IPY_MODEL_f40f7708e2fc495b8b216f76617773a5"}},"2dd5e840ee7040629ed67684ea5568f7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3017ed2331541739c2f89486ad5aa9c","placeholder":"​","style":"IPY_MODEL_8a426c2a4c5e45028b9d81d04e685537","value":"wikitext-2-raw-v1/train-00000-of-00001.p(…): 100%"}},"03e3e850ae294658a74b5fea19171910":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2775d47181544c62af7c0627b2912770","max":6357543,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cb13f4cc7cd540e484d91f3511c363a4","value":6357543}},"d48d9fed58b042fd8f7f8b1db558d6cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0324f662c38749b08a7b7b34a0f79016","placeholder":"​","style":"IPY_MODEL_f2442f1daf66477d9fba629617657b3b","value":" 6.36M/6.36M [00:00&lt;00:00, 380kB/s]"}},"f40f7708e2fc495b8b216f76617773a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3017ed2331541739c2f89486ad5aa9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a426c2a4c5e45028b9d81d04e685537":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2775d47181544c62af7c0627b2912770":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cb13f4cc7cd540e484d91f3511c363a4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0324f662c38749b08a7b7b34a0f79016":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2442f1daf66477d9fba629617657b3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c014428653964bedb8df37405befd1eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2945817746814dbd9bf272b8e745ee56","IPY_MODEL_80677cab0dba4c518d24621ea0ced533","IPY_MODEL_3d750b36f7024a8d8198488ebc12677c"],"layout":"IPY_MODEL_da976b73684f4a889628dae06bfe6a92"}},"2945817746814dbd9bf272b8e745ee56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4cf85073f942483e8f1c3e9da6431903","placeholder":"​","style":"IPY_MODEL_a997e61156f34941a6e48e1f1ad99710","value":"wikitext-2-raw-v1/validation-00000-of-00(…): 100%"}},"80677cab0dba4c518d24621ea0ced533":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6930f87ad9334f7390f37360f97552ec","max":657209,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73c3e4e773454101b2605202d1eb3469","value":657209}},"3d750b36f7024a8d8198488ebc12677c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eff942a73f494f0c86d914f3a6c537a2","placeholder":"​","style":"IPY_MODEL_6bb792e06269476f83f2c932551fe967","value":" 657k/657k [00:00&lt;00:00, 593kB/s]"}},"da976b73684f4a889628dae06bfe6a92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cf85073f942483e8f1c3e9da6431903":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a997e61156f34941a6e48e1f1ad99710":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6930f87ad9334f7390f37360f97552ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73c3e4e773454101b2605202d1eb3469":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eff942a73f494f0c86d914f3a6c537a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bb792e06269476f83f2c932551fe967":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34c03d4782324205bfbf8d117fccfa5a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a8b1eb1410654599858d9007b1ad3d7a","IPY_MODEL_6e21860a85a742a0b1a8caf40809e015","IPY_MODEL_c5524ae61c69488bb48a1e0fbc164656"],"layout":"IPY_MODEL_cdd345a333bd4fc79423c8c72baf1add"}},"a8b1eb1410654599858d9007b1ad3d7a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b2c10e7d04440b99e8bb49cd5b715da","placeholder":"​","style":"IPY_MODEL_5323644284f24faba22998441b8529cb","value":"Generating test split: 100%"}},"6e21860a85a742a0b1a8caf40809e015":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab393b7e4c434f648d2e01af681d6f5c","max":4358,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcad8f1e2e0647e591e17cf78693dfda","value":4358}},"c5524ae61c69488bb48a1e0fbc164656":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e873beb31f749549dec03ea5c42e3ce","placeholder":"​","style":"IPY_MODEL_5f1d269188604cf9a675c864022432f6","value":" 4358/4358 [00:00&lt;00:00, 90072.96 examples/s]"}},"cdd345a333bd4fc79423c8c72baf1add":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b2c10e7d04440b99e8bb49cd5b715da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5323644284f24faba22998441b8529cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab393b7e4c434f648d2e01af681d6f5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcad8f1e2e0647e591e17cf78693dfda":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9e873beb31f749549dec03ea5c42e3ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f1d269188604cf9a675c864022432f6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f0fd4a382fa42be97a38577236c0535":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b68adae78f0a4e40aa3bf642d9436af3","IPY_MODEL_b67632bf9ade431bb053d4e23e5cceac","IPY_MODEL_c31ead4ac1e144c0a954812f7054ec84"],"layout":"IPY_MODEL_05d79e7fd45e4b2fb410eb9c5fda5501"}},"b68adae78f0a4e40aa3bf642d9436af3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2aca4ff7dd8b497a95e0550388eb10a1","placeholder":"​","style":"IPY_MODEL_d4009a9820df4e779f75c73e177cdce2","value":"Generating train split: 100%"}},"b67632bf9ade431bb053d4e23e5cceac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_52b851ebbe9f4a289d743d5df9192c22","max":36718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8edc3b600e9f4ad6bc26e5fe707626ff","value":36718}},"c31ead4ac1e144c0a954812f7054ec84":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8da3ff10f4a043f79556814ac22fe643","placeholder":"​","style":"IPY_MODEL_d3620c64a7d44e7eab732d650cdc9f96","value":" 36718/36718 [00:00&lt;00:00, 669604.92 examples/s]"}},"05d79e7fd45e4b2fb410eb9c5fda5501":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2aca4ff7dd8b497a95e0550388eb10a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4009a9820df4e779f75c73e177cdce2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"52b851ebbe9f4a289d743d5df9192c22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8edc3b600e9f4ad6bc26e5fe707626ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8da3ff10f4a043f79556814ac22fe643":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3620c64a7d44e7eab732d650cdc9f96":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de8b4726861f4c0f87d92e353bc46088":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3e7d6b15b114301a8ea8f160fceee46","IPY_MODEL_55695008e0434a918b61c1f3d94ae8d2","IPY_MODEL_f8925b0a3c72472ea4dbb7db1bb79950"],"layout":"IPY_MODEL_3363ba9775434b75bc1ad4c1c2a7fae4"}},"e3e7d6b15b114301a8ea8f160fceee46":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c27c6e01f164cf39b0e7b8b1595fe29","placeholder":"​","style":"IPY_MODEL_8317c8cb7699418f95726f30bd1f96e8","value":"Generating validation split: 100%"}},"55695008e0434a918b61c1f3d94ae8d2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_507f16fc46d84b4dbc4cbe2f75a894eb","max":3760,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3120240cd5d43bf99220cfda96f5421","value":3760}},"f8925b0a3c72472ea4dbb7db1bb79950":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1363ffd7492e4fe5965f706b1b3d9327","placeholder":"​","style":"IPY_MODEL_4e6bfe6c655e4fbbbcc887a07bb32b51","value":" 3760/3760 [00:00&lt;00:00, 239543.46 examples/s]"}},"3363ba9775434b75bc1ad4c1c2a7fae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c27c6e01f164cf39b0e7b8b1595fe29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8317c8cb7699418f95726f30bd1f96e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"507f16fc46d84b4dbc4cbe2f75a894eb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3120240cd5d43bf99220cfda96f5421":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1363ffd7492e4fe5965f706b1b3d9327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e6bfe6c655e4fbbbcc887a07bb32b51":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# MAth import kar rhe hain qki isme log, exp, sqrt etc hota hai...\n","# jo word2vec me kaam aa sakta hai\n","# (loss calculate nd negative sampling ka math)\n","import math\n","\n","# Torch ka use hoga tensor banane (torch.tensor, torch.randn)\n","# GPU pe computation karne ke liye (.to(device))\n","import torch\n","\n","# Yaha neural network banana hai to fir `torch.nn` chahiye\n","# nn use karke mai layers banaunga 'nn.Linear', 'nn.Embedding' etc.\n","# word2vec me mainly nn.Embedding layer use karenge jo wods ko vectors me convert karega\n","import torch.nn as nn\n","\n","# model ko train karne ke liye optimizer (Adam, SGD etc) v chahiye\n","# w2v me v hm har batch ke baad embeddings update karenge optimizer se\n","import torch.optim as optim\n","\n","# Ab data ko handle karna hai\n","# Dataset → ek custom class banayenge jo bataega:\n","#  • total samples (__len__)\n","#  • ith sample (__getitem__)\n","\n","# DataLoader → ye Dataset se data uthata hai:\n","#  • batches banata hai, shuffle krta hai, training loop clean bana deta hai\n","\n","# w2v me hm input word + context/negative samples ko properly batches me\n","# feed karne ke liye use karenge\n","from torch.utils.data import Dataset, DataLoader\n","\n","# yaha mujhe words ki frequency gin ni hai\n","# Counter - ek list of words lega → frequency of each word return karega\n","# w2v me useful hai:\n","# • vocab banane ke liye\n","# • rare words ko filter karne ke liye\n","# • negative sampling ke probabilities set karne ke liye(frequent words zyada chance, rare ko kam)\n","from collections import Counter\n","\n","# Ab hugging face ka dataset library use karunga\n","# load_dataset se real world text dataset kiad jarege\n","from datasets import load_dataset"],"metadata":{"id":"4Av7vFEF6VJ9","executionInfo":{"status":"ok","timestamp":1764008042158,"user_tz":-330,"elapsed":6980,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Setup Agnostic Device"],"metadata":{"id":"O37t2LuF_oKX"}},{"cell_type":"code","source":["'''\n","ab yaha mai check karungna ki kon sa device available hai.\n","torch.cuda.is_available chck karega ki kya mere paas GPU ka access hai ya nhi\n","agar GPU nhi hai to default device CPU ko bana dega nd wha saara kaam hoga\n","'''\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(\"Using device: \", device)"],"metadata":{"id":"3BhE0SP9_tfX","executionInfo":{"status":"ok","timestamp":1764008042241,"user_tz":-330,"elapsed":33,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47e4cd06-7b60-4a1b-a9b2-608da4b041e3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device:  cuda\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"id":"vspeKt47iu4y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Text Data"],"metadata":{"id":"I_qPk0UR_2sT"}},{"cell_type":"code","source":["'''\n","ab mai dataset library ke `load_dataset` ki madad se 2 data lunga.\n","'wikitext; → cleaned version hai\n","'wikitext-2-raw-v1': uncleaned version hai jo original k kaafi close hai\n","                     (includes punctuations, caps etc)\n","'split=train': matlab sirf training wala data load karunga...test wala nhi\n","'''\n","dataset = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\n","\n","# ab mujhe text dekhna hai dataset se...isliye mai \"text\" column uthaa lunga\n","lines = dataset[\"text\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449,"referenced_widgets":["22a9e09887d941a1b215f92ebc0617e5","fb195e7828e8461b88355c37424d8742","8835be5ea9f348788f506437ecfa1572","c0514ee7082d4544ad67acd0ec0c2430","522ba1b89e44462f9b5f24405c43bc4f","83534db02d574382a0e95ce207f0a112","f3ae4abb4f814b90a7cdb090c1d6318c","2ed3b83276c3444da1c7f42b086a043c","213f220d1b1c4381b11f18cd7793da62","5ac80fe9491f4f58af910ad5a9bb69d9","ff808f9245d740c68dd6b5bddb4c55d6","d2a6f3eb6f884a7c80b346e6b8bf26bd","4519142bad8c4438a73da0e76ff7eaa5","f619d79994ee489284b3e32136fd8cf7","0655089100794e2c93fcad84b4e5d3f3","c16bc80d1fc8492cb71f478d522fe632","55402d823cea42ee8520ec9a30a9afca","aed6482c8ee64189a81a74c67511d19b","065a95c5cd784290897133663ecf3796","203c03d58abf462b95d1ad9911903839","5a4b59c59fdc4e6ba04f0029e07e11f6","a0fe57defced4c72b9c22c7bef75d747","e08d0662d69f4efc90b7715ca862cc70","2dd5e840ee7040629ed67684ea5568f7","03e3e850ae294658a74b5fea19171910","d48d9fed58b042fd8f7f8b1db558d6cd","f40f7708e2fc495b8b216f76617773a5","a3017ed2331541739c2f89486ad5aa9c","8a426c2a4c5e45028b9d81d04e685537","2775d47181544c62af7c0627b2912770","cb13f4cc7cd540e484d91f3511c363a4","0324f662c38749b08a7b7b34a0f79016","f2442f1daf66477d9fba629617657b3b","c014428653964bedb8df37405befd1eb","2945817746814dbd9bf272b8e745ee56","80677cab0dba4c518d24621ea0ced533","3d750b36f7024a8d8198488ebc12677c","da976b73684f4a889628dae06bfe6a92","4cf85073f942483e8f1c3e9da6431903","a997e61156f34941a6e48e1f1ad99710","6930f87ad9334f7390f37360f97552ec","73c3e4e773454101b2605202d1eb3469","eff942a73f494f0c86d914f3a6c537a2","6bb792e06269476f83f2c932551fe967","34c03d4782324205bfbf8d117fccfa5a","a8b1eb1410654599858d9007b1ad3d7a","6e21860a85a742a0b1a8caf40809e015","c5524ae61c69488bb48a1e0fbc164656","cdd345a333bd4fc79423c8c72baf1add","5b2c10e7d04440b99e8bb49cd5b715da","5323644284f24faba22998441b8529cb","ab393b7e4c434f648d2e01af681d6f5c","bcad8f1e2e0647e591e17cf78693dfda","9e873beb31f749549dec03ea5c42e3ce","5f1d269188604cf9a675c864022432f6","4f0fd4a382fa42be97a38577236c0535","b68adae78f0a4e40aa3bf642d9436af3","b67632bf9ade431bb053d4e23e5cceac","c31ead4ac1e144c0a954812f7054ec84","05d79e7fd45e4b2fb410eb9c5fda5501","2aca4ff7dd8b497a95e0550388eb10a1","d4009a9820df4e779f75c73e177cdce2","52b851ebbe9f4a289d743d5df9192c22","8edc3b600e9f4ad6bc26e5fe707626ff","8da3ff10f4a043f79556814ac22fe643","d3620c64a7d44e7eab732d650cdc9f96","de8b4726861f4c0f87d92e353bc46088","e3e7d6b15b114301a8ea8f160fceee46","55695008e0434a918b61c1f3d94ae8d2","f8925b0a3c72472ea4dbb7db1bb79950","3363ba9775434b75bc1ad4c1c2a7fae4","3c27c6e01f164cf39b0e7b8b1595fe29","8317c8cb7699418f95726f30bd1f96e8","507f16fc46d84b4dbc4cbe2f75a894eb","f3120240cd5d43bf99220cfda96f5421","1363ffd7492e4fe5965f706b1b3d9327","4e6bfe6c655e4fbbbcc887a07bb32b51"]},"id":"vkOZoz12_6Ya","executionInfo":{"status":"ok","timestamp":1764008132688,"user_tz":-330,"elapsed":6212,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"4fe371fa-7a2c-456a-aa66-5ec5f55267b4"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["README.md: 0.00B [00:00, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22a9e09887d941a1b215f92ebc0617e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a6f3eb6f884a7c80b346e6b8bf26bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e08d0662d69f4efc90b7715ca862cc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c014428653964bedb8df37405befd1eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34c03d4782324205bfbf8d117fccfa5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f0fd4a382fa42be97a38577236c0535"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de8b4726861f4c0f87d92e353bc46088"}},"metadata":{}}]},{"cell_type":"code","source":["# ab word2vec ke liye corpus ready karna hai, let's do it\n","\n","# ek khaali list banaunga jisme words (tokens) jama karunga:\n","# [\"this\", \"is\", \"first\", \"sentence\", \"and\", \"ye\", \"dusra\", \"hai\", ...]\n","tokens = []\n","\n","# ab text ke har ek line ko line-by-line lunga aur...\n","for line in lines:\n","\n","\n","  # uske aage-peeche ke extra spaces ya new line characters hata dunga...\n","  line = line.strip()\n","\n","  # aur jo empty line hai...usse ignore kar dunga\n","  # wikitext me heading kuch aisi hai: '===heading==='\n","  # usse v hata dunga (ignore karke)\n","  if not line or line.startswith('='):\n","    continue\n","\n","  # ab mere paas jo v lines bachi hain...usko mai lower me conver karunga,\n","  # whitespace ke basis pe split kar dunga...\n","  # aur list ke andar isse add kartaa jaaunga.\n","  tokens.extend(line.lower().split())\n","\n","print(f\"Number of tokens = {len(tokens)}\")\n","\n","# To keep training reasonable fast for demo, use only first N tokens\n","max_tokens = 4000000\n","tokens = tokens[:max_tokens]\n","print(\"Total tokens used:\", len(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_lz_NIsWAT1g","executionInfo":{"status":"ok","timestamp":1764008146943,"user_tz":-330,"elapsed":843,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"84feec9e-dedd-43b0-d58f-9711c8560e70"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tokens = 2007146\n","Total tokens used: 2007146\n"]}]},{"cell_type":"markdown","source":["# Build Vocabulary"],"metadata":{"id":"wO9W_RF4BNp3"}},{"cell_type":"code","source":["# av tak hmlg words nikaale hain jo training me dala jaayega.\n","# ab yaha se mai unn words ka ek dict banaunga jo w2v me use hoga\n","\n","# ye ek limit hai. Koi v word 5 baar se kam aaya to usko ignore karenge (rare words)\n","# list me aane ke liye wo 5 ya usse zyada baar repeat hona chahiye\n","min_freq = 5\n","\n","# let's count ki kon sa word kitni baar aaya\n","# { \"the\": 50000, \"of\": 30000, \"data\": 1200, \"india\": 800, ... }\n","word_counts = Counter(tokens)\n","\n","\n","# ab mujhe vocab banani hai. sabse phle ek special token rakhunga <unk> ke\n","# naam se, baaki me sirf whi words rakhunga ji 'min_freq' ya usse zyada baar\n","# aaye hain\n","# Result: [\"<unk>\", \"the\", \"of\", \"and\", \"to\", \"in\", ...]\n","vocab = [\"<unk>\"] + [w for w, c in word_counts.items() if c >= min_freq]\n","\n","'''\n","ab mujhe words → indices mapping banana hoga qki model ko words se zyada\n","number pasand hai\n","\n","result = word_to_idx = {\"<unk>\": 0, \"the\": 1, \"of\": 2, \"and\": 3,...}\n","\n","ye mapping important hai.\n","• jab hum koi sentence token list me rakhte hain, to hr word ko uske\n","  index me convert karenge:\n","  \"the\" → 1, \"india\" → maybe 365, unknown/new word → 0 (<unk>)\n","'''\n","word_to_idx = {w: i for i, w in enumerate(vocab)}\n","\n","# ho sakta hai ki index ke basis pe word chahiye hoga...to index to word\n","# ka dictionary v bana leta hu...(debugging, ya nearest neighbour)\n","# Result: ids_to_word = {0: \"<unk>\", 1: \"the\", 2: \"of\", ...}\n","ids_to_word = {i: w for w, i in word_to_idx.items()}\n","\n","\n","# Ab main count kar raha hoon ki final vocabulary me total kitne unique\n","# tokens aaye.\n","# isme unknown v included hai...means: 1 (unk) + total unique words\n","vocab_size = len(vocab)\n","print(f\"Vocabulary size = {vocab_size}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bicdkK4BPjg","executionInfo":{"status":"ok","timestamp":1764008150514,"user_tz":-330,"elapsed":18,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"2699d290-96f7-481c-f138-5b8a1079c242"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size = 20889\n"]}]},{"cell_type":"code","source":["# yaha se word → number me convert hota hai\n","\n","'''\n","Ab mere paas tokens me saare words sequence me pade hain...lekin model\n","ko words nhi, numbers (indices) chahiye. To chalo har word ko uske\n","index me conver karte hain\n","'''\n","\n","# token se ek word (w) lo, aur dict se uss word ka index nikaalo agar hai to\n","# warna wha 0 daal do (unknown)\n","corpus_indices = [word_to_idx.get(w, 0) for w in tokens]\n","corpus_indices[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XDKRf48rB3SU","executionInfo":{"status":"ok","timestamp":1764008153377,"user_tz":-330,"elapsed":203,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"f1e564fe-15ea-4567-deb7-49df284b7d15"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3, 4, 5, 0, 6, 7, 8, 5]"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Dataset: Skipgram with negative sampling"],"metadata":{"id":"H64yK0xiCNV9"}},{"cell_type":"code","source":["# yaha mai PyTorch Dataset bana raha hu...jo specifically skipgram w2v ke liye hai\n","\n","class SkipGramDataset(Dataset):\n","  def __init__(self, indices, vocab_size, window_size=2, num_negatives=5):\n","    \"\"\"\n","        indices      : poora corpus numbers me: corpus_indices (word ids ki list).\n","        vocab_size   : kitne unique words hain (embedding matrix ka size)\n","        window_size  : center word ke aas-paas kitne context words dekhne hain\n","        num_negatives: har (center, context) ke saath kitne negative words sample karne hain.\n","    \"\"\"\n","    self.indices = indices\n","    self.vocab_size = vocab_size\n","    self.window_size = window_size\n","    self.num_negatives = num_negatives\n","\n","  # BUILD (CENTER, CONTEXT) PAIR\n","  # (center, context) store karne ke liye ek list bana rhe hain\n","    pairs = []\n","\n","    # hm har position wale word ko ek possible center maan rhe hain\n","    # center pos = 0, 1, 2...len(indices) - 1\n","    for center_pos in range(len(indices)):\n","\n","      # iss index pe jo v word hai...wo mera center word hai (assume)\n","      center = indices[center_pos]\n","\n","\n","      # yaha hm sliding window bana rhe hain\n","      # window size 2 hai to...\n","      # offset hoga -2, -1, 0, +1, +2 ← mtlb `2 left, 1 left, khud center, 1 right, 2 right`\n","      for offset in range(-window_size, window_size + 1):\n","\n","\n","        # yaha mera literal context position hai\n","        # 8, 9, 10, 11, 12\n","        context_pos = center_pos + offset\n","\n","        # ye safety check hai out of range exception ka.\n","        # jo v value corpus ke bahar jaa rha hai...usko ignore karo\n","        if context_pos < 0 or context_pos >= len(indices):\n","          continue\n","\n","\n","        # center ka context nhi maan na hai...usko ignore karna hai\n","        if context_pos == center_pos:\n","          continue\n","\n","        # woh neighbor word ka index utha liya.\n","        context = indices[context_pos]\n","\n","        # pair me append kar diye ek tuple of center and context\n","        pairs.append((center, context))\n","\n","\n","    self.pairs = pairs\n","    print(\"Number of (center, context) pairs:\", len(self.pairs))\n","\n","\n","    # BUILD NEGATIVE SAMPLING DISTRIBUTION\n","\n","    # Count how ofter each word index appear\n","    # Result example: {0: 1234, 1: 56789, 2: 1000, ...}\n","    word_freqs = Counter(indices)\n","\n","    # Make a tensor of frequencies with length vocab_size\n","    # Result: [1234, 56789, 1000...] in the form of float tensor\n","    # in case index aaya hi nhi to Counter default value 0 dega\n","    freqs = torch.tensor(\n","        [word_freqs[i] for i in range(vocab_size)],\n","        dtype = torch.float\n","    )\n","\n","    # As in word2vec paper: raise to power 0.75\n","    # Logic: agar word ka freq zyada hai (is, of, the, etc)..to wo distribution ko dominate krta hai\n","    # 0.75 power frequent word ko thoda compress karta hai nd rare word ko thoda chance mil jaata hai\n","    freqs = freqs ** 0.75\n","\n","    # Normalize to make it a probability distribution\n","    # ab ye freqs ko probability me convert karna hai\n","    # isse ek tensor milega jiska sum ~ 1 hoga\n","    self.neg_sampling_dist = freqs / freqs.sum()\n","\n","\n","  # total kitne (center, context) pair hain...wo dekh rha hai\n","  def __len__(self):\n","    return len(self.pairs)\n","\n","  # ith index position wale dataset me value kaisa dikhega...ye pata chalega\n","  def __getitem__(self, idx):\n","    center, context = self.pairs[idx]\n","\n","    # Sample negative word indices from the vocab\n","    # Means...mujhe vocab se num_negatives words sample kar do...\n","    # jaha har word ka prob. neg_sampling_dist se decide ho.\n","    # Replacements allowed hai...means same -ve words diff. times allowed hai\n","    negatives = torch.multinomial(\n","        self.neg_sampling_dist,\n","        self.num_negatives,\n","        replacement = True\n","    )\n","\n","\n","    # ab saare indices ko proper pytorch tensors me bana deta hu...\n","    # taaki directly model me feed ho sake.\n","    # isko long me daale hain qki nn.Embedding ko integer (long) chahiye hote hain\n","    center = torch.tensor(center, dtype=torch.long)\n","    context = torch.tensor(context, dtype=torch.long)\n","    negatives = negatives.long()\n","\n","    # last me (center, context, negatives) ko return karna hai\n","    return center, context, negatives"],"metadata":{"id":"57yjh0-DCRvU","executionInfo":{"status":"ok","timestamp":1764008159787,"user_tz":-330,"elapsed":11,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Create dataset and dataloader\n","dataset = SkipGramDataset(corpus_indices, # poore corpus ka integer sequence [12, 45, 7, 89, ...]\n","\n","                          # kitne unique words hai\n","                          # negative sampling ke liye frequency tensor ka size decide hoga\n","                          # prob. dist. (-ve sampling dist) ka shape set hoga\n","                          vocab_size=vocab_size,\n","\n","                          # center word ke left-right 2 words context ke liye jaayenge\n","                          # ctr = brown, ctx = [the, quick, fox, jumps]\n","                          window_size=2,\n","\n","                          # har (ctr, ctx) positive pair ke saath 5 random -ve words aayenge training sample se\n","                          num_negatives=5)\n","\n","dataloader = DataLoader(dataset, # dataset se items uthata hai\n","\n","                        # 512 samples milega...\n","                        # 512 center words\n","                        # 512 positive context words\n","                        # 512 x 5 negative words\n","                        batch_size=512,\n","\n","                        # dataset ke indices ko reshuffle kar do\n","                        shuffle=True\n","                        )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvJjtKV-91FI","executionInfo":{"status":"ok","timestamp":1764008174444,"user_tz":-330,"elapsed":2684,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"d4042a34-327f-4fcc-e1d0-90106ecc6c94"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of (center, context) pairs: 8028578\n"]}]},{"cell_type":"code","source":["# MODEL: WORD2VEC SKIPGRAM + NEGATIVE SAMPLING\n","class Word2VecSkipGram(nn.Module):\n","  def __init__(self,\n","               vocab_size, # kitne words hain\n","               embed_dim): # har word ka vector kitne dimension ka hoga (e.g. 100, 300)\n","    super().__init__()\n","\n","\n","    '''\n","    Har word kv center banta hai aur kv context.\n","    w2v me in dono roles ke liye alag embeddings rakhe jaate hain.\n","    '''\n","    # embeddings when word acts as the center\n","    self.in_embed = nn.Embedding(vocab_size, embed_dim)\n","    # embeddings when word acts as the context\n","    self.out_embed = nn.Embedding(vocab_size, embed_dim)\n","\n","\n","\n","    # ab embeddings ko initial value deke initialize karna hai\n","\n","    # ye chhota sa number hai. agar embed_dim 100 hai...\n","    # to initial range [-0.005, 0.005] ke beech hoga\n","    init_range = 0.5 / embed_dim\n","\n","    # center embeddings ko chhoti random values de rhe hain.\n","    # agar sab 0 se start hua to saare word initially same honge...jo ki nhi hai\n","    # random init se har word thoda alag start karta hai.\n","    self.in_embed.weight.data.uniform_(-init_range, init_range)\n","\n","\n","    # context embeddings ko zero se init kar rahe ho.\n","    # grad aayega to update hota rahega\n","    self.out_embed.weight.data.zero_()\n","\n","    # ye design choice hai...in embed ko randm nd output embed ko 0, gradually learn\n","\n","\n","\n","  # forward me hm directly loss compute karenge\n","  # ye typical classifier jaisa logits return nhi krega\n","  # yehi pe sigmoid + log + loss sab ho jaayega\n","  def forward(self, center_words, pos_context, neg_context):\n","      \"\"\"\n","      center_words : (batch,) → har entry ek word index hai\n","      pos_context  : (batch,) → har center ke liye ek +ve context word index\n","      neg_context  : (batch, num_negatives) → har center ke liye k -ve word indices\n","      \"\"\"\n","\n","      # Lookup embeddings\n","      # (batch, num_negatives, embed_dim) == (B, K, D)\n","      center_emb = self.in_embed(center_words) #(batch,) → (batch, embed_dim)\n","      pos_emb = self.out_embed(pos_context) # (batch, ) → (batch, embed_dim)\n","      neg_emb = self.out_embed(neg_context) # (batch, num_negatives) → (batch, num_negatives, embed_dim)\n","\n","\n","      # POSITIVE PART → “ye real pair hai, score high karo”\n","      # center • positive context (element wise multiply)\n","      # dot product ke baad dim=1 (embedding ke features) ka sum kar do; dim=0 batch hai\n","      pos_score = torch.sum(center_emb * pos_emb, dim=1)\n","\n","      # We want sigmoid(pos_score) close to 1\n","      # sigmoid ko [0,1] probability me convert karna hai:\n","      # 1) high dot product → sigmoid ≈ 1\n","      # 2) low dot product → sigmoid ≈ 0\n","      # + 1e-10: for stability → kahi sigmoid bilkul 0 ho jaaye to log(0) na ho.\n","      pos_loss = -torch.log(torch.sigmoid(pos_score) + 1e-10)\n","\n","\n","\n","\n","\n","      # NEGATIVE PART\n","\n","      # center_emb: (batch, embed_dim) -> (batch, embed_dim, 1)\n","      # neg_emb   : (batch, num_neg, embed_dim)\n","      # yaha .unsqueeze(2) karne se center_emb nd neg_embed dono same dimension ka hoga → matrix multiplication possible → no shape mismatch error\n","\n","      # bmm = match matrix multiplication\n","      # har batch ke liye\n","      # • neg_emb[b] shape: (K, D)\n","      # • center_emb[b] shape: (D, 1)\n","      # • result: (K, 1) = har -ve word ja dot product center ke saath\n","      # Overall result (including batch also) → (B, K, 1)\n","      # .unsqueeze() karke last dim (1) ho hata do → (B, K)\n","      neg_score = torch.bmm(neg_emb, center_emb.unsqueeze(2)).squeeze()\n","\n","\n","\n","      # We want sigmoid(neg_score) close to 0\n","      # yaha 1-sigmoid(neg_score) ~ 1\n","      # log(1-sigmoid(...)) ~ 0\n","      # .sum() isliye qki har example me multiple -ve words hain (k negatives)\n","      # unn sab ke losses ko sum karo and last me total -ve loss per center de do.\n","      neg_loss = -torch.sum(\n","          torch.log(1 - torch.sigmoid(neg_score) + 1e-10),\n","          dim=1\n","      )\n","\n","      # combine and average over batch\n","      loss = (pos_loss + neg_loss).mean()\n","\n","      return loss"],"metadata":{"id":"5zvZy0HC-OlP","executionInfo":{"status":"ok","timestamp":1764008176446,"user_tz":-330,"elapsed":21,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Training\n","\n","# embedding dimension decide kare hain...har word ko 100 length ke vector se represent karenge\n","embed_dim = 50 # 200, 300...are common\n","\n","\n","# ab model bana ke usko sahi device pe send kr rahe hain (CPU/GPU)\n","# w2v do embedding matrix banayega:\n","# • in_embed = (vocab_size, embed_dim)\n","# • out_embed = (vocab_size, embed_dim)\n","model = Word2VecSkipGram(vocab_size, embed_dim).to(device)\n","\n","# adam adaptive optimizer hai...lr ko har param ke liye smart tareeke se adjust karta hai\n","optimizer = optim.Adam(model.parameters(), lr=0.002)\n","\n","# mai poora dataset model ko 10 baar dikhaaunga\n","num_epochs = 1\n","\n","\n","# for each epoch from 0 - 49...\n","for epoch in range(num_epochs):\n","\n","  # training mode me jaao\n","  # agar batchnorm ya dropout hota to unka behaviour change ho jaata\n","  # lekin iss case me nhi hai to as such change nhi hoga\n","  model.train()\n","\n","  # total loss ko start me 0 se initialize karo\n","  total_loss = 0.0\n","\n","\n","  # yaha ctr, ctx, neg ko sahi device pe send karo jaha data available hai\n","  # warna error milega...expected device cuda but got gpu\n","  for center, context, negatives in dataloader:\n","    center = center.to(device)\n","    context = context.to(device)\n","    negatives = negatives.to(device)\n","\n","    # pichle batch ke gradients ko phle clear kar do\n","    # pytorch me gradients by default accumulate hote hain...\n","    # agar ye nhi kara to backprop prev. gradient ke upar add ho jaayega\n","    optimizer.zero_grad()\n","\n","    # ab forward pass karo:\n","    # • embeddings lookup\n","    # • positive score\n","    # • negative score\n","    # • pos_loss, neg_loss\n","    # • combine → mean\n","    # loss ek scalar tensor hoga (ek avg value)\n","    loss = model(center, context, negatives)\n","\n","    # ab loss pe backprop karo nd loss ke hisab se har wt ka gradient nikaal do.\n","    loss.backward()\n","\n","    # ab gradient ke hisaab se weight ko update karo\n","    # Adam → `param = param - lr * (processed_gradient)`\n","    optimizer.step()\n","\n","    # iss epoch ke liye har batches ka total loss add karo\n","    total_loss += loss.item()\n","\n","  print(f\"Epoch {epoch+1}/{num_epochs}, loss = {total_loss: .4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1tIbV_rA6MX","executionInfo":{"status":"ok","timestamp":1764008654648,"user_tz":-330,"elapsed":463598,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"61df6aad-14d1-48db-f1df-b6980254707d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1, loss =  36805.1204\n"]}]},{"cell_type":"code","source":["# Use the learned embeddings: find similar words\n","\n","# yaha sirf inference kar rhe hain...training nhi\n","@torch.no_grad()\n","\n","# ek word ko embedding vector me convert karna hai\n","def get_word_embedding(word: str) -> torch.Tensor:\n","\n","  # phle word ka index nikaal lo (from available vocab dict)\n","  idx = word_to_idx.get(word, 0)\n","\n","  # ab iss index wale row ka weight nikaal lunga in_embed se...jo model me hai,\n","  # aur usse return kar dunga\n","  return model.in_embed.weight[idx]\n","\n","\n","# again inference mode hai, grad nhi chahiye\n","@torch.no_grad()\n","\n","# ek word ke top-k similar word predict karna hai\n","def most_similar(query_word: str, top_k: int = 5):\n","\n","  # agar word vocab me nhi hai to bata do ki similar word predict nhi ho paayega\n","  # Ab main us word ka actual embedding vector le raha hoon.\n","  if query_word not in word_to_idx:\n","    print(f\"'{query_word}' is not in the vocab.\")\n","    return\n","\n","  # query word ka embedding lo jo ek tensor hoga...wo store kar lo\n","  # shape: (embed_dim,)\n","  query_vec = get_word_embedding(query_word)\n","\n","  # iss all_embs me saare words ke embeddings ek saath aa gaye\n","  # Shape: (vocab_size, embed_dim)\n","  all_embs = model.in_embed.weight\n","\n","  # Cosine similarity: (v . w / (|v||w|))\n","  # Shape: (vocab_size, embed_dim) • (embed_dim,) = (vocab_size,)\n","  # all_emb.norm(...) = har word embedding ki L2 norm\n","  # query_vec.norm(...) = query word ki L2 norm\n","  # L2 norm = Euclidean norm = ||x||₂ = √(x₁² + x₂² + ... + xₙ²)\n","  #   dim=0 → “kitne examples / words hain” ka axis\n","  # dim=1 → “har example ka vector / features” ka axis\n","  sims = torch.matmul(all_embs, query_vec) / (\n","      all_embs.norm(dim=1) * query_vec.norm() + 1e-10\n","  )\n","\n","  values, indices = torch.topk(sims, top_k + 1) # +1 to include the word itself, will skip it later\n","  print(f\"\\nWords most similar to '{query_word}':\")\n","\n","  # yaha topk+1 similarity ke liye (score, idx) use kro and uspe iterate karo\n","  for score, idx in zip(values, indices):\n","\n","    # index ki madad se uska word nikaal lo (ids_to_word) dict ka use karke\n","    w = ids_to_word[idx.item()]\n","\n","    # Agar query word khud word me aaye to usse ignore karo\n","    if w == query_word:\n","      continue\n","    print(f\"{w:15s} similarity = {score.item(): .3f}\")"],"metadata":{"id":"-D6VVX72HCAy","executionInfo":{"status":"ok","timestamp":1764008654652,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["most_similar(\"king\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Koq0b-lRUj2","executionInfo":{"status":"ok","timestamp":1764008654756,"user_tz":-330,"elapsed":103,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"2cb653bf-4986-4725-f9b8-b332a879ab7d"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words most similar to 'king':\n","queen           similarity =  0.834\n","emperor         similarity =  0.826\n","lord            similarity =  0.820\n","edward          similarity =  0.797\n","henry           similarity =  0.788\n"]}]},{"cell_type":"code","source":["most_similar(\"queen\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eLl5S-fRYuV","executionInfo":{"status":"ok","timestamp":1764008654771,"user_tz":-330,"elapsed":5,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"f62175a3-d6a3-49b7-938d-9fdd1f743e99"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words most similar to 'queen':\n","king            similarity =  0.834\n","lord            similarity =  0.820\n","edward          similarity =  0.817\n","rothschild      similarity =  0.817\n","kennedy         similarity =  0.816\n"]}]},{"cell_type":"code","source":["most_similar(\"london\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fQpTxJ4gRaJ8","executionInfo":{"status":"ok","timestamp":1764008654778,"user_tz":-330,"elapsed":6,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"0a324d57-484e-4082-b101-b7503c28623b"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Words most similar to 'london':\n","louisiana       similarity =  0.881\n","brazil          similarity =  0.877\n","1839            similarity =  0.846\n","proctor         similarity =  0.845\n","scotland        similarity =  0.842\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1nzXi-WSFSlV","executionInfo":{"status":"ok","timestamp":1764008804108,"user_tz":-330,"elapsed":21650,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"30bad228-61c8-4120-ab84-d638cfab879d"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/\"Colab Notebooks\"/\"GenAI with Python and PyTorch\"/\"Chapter 3\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V-fg4PZGFWgD","executionInfo":{"status":"ok","timestamp":1764008808681,"user_tz":-330,"elapsed":1634,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"29f6466c-8e22-4fdc-b38d-3f68837a0549"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/GenAI with Python and PyTorch/Chapter 3\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"word2vec_state.pth\")"],"metadata":{"id":"2p3K638HFY-3","executionInfo":{"status":"ok","timestamp":1764008809205,"user_tz":-330,"elapsed":522,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}}},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":["# Page 70 (from book) - Using gensim library\n"],"metadata":{"id":"LbhnV7LJ6SDt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Ql_3BUBNEGR"},"outputs":[],"source":["import re\n","import pandas as pd\n","import numpy as np\n","import nltk\n","from sklearn.datasets import fetch_20newsgroups"]},{"cell_type":"code","source":["nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')"],"metadata":{"id":"CdT0YXYgNIki","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763866254073,"user_tz":-330,"elapsed":870,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"13b201ca-29f0-4448-e1fb-426fdbb2b09e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["stop_words = nltk.corpus.stopwords.words('english')"],"metadata":{"id":"bz_mSsafNIhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def normalize_document(doc):\n","  doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n","  doc = doc.lower()\n","  doc = doc.strip()\n","\n","  tokens = nltk.word_tokenize(doc)\n","\n","  filtered_tokens = [token for token in tokens if token not in stop_words]\n","\n","  doc = ' '.join(filtered_tokens)\n","\n","  return doc\n","\n","normalize_corpus = np.vectorize(normalize_document)"],"metadata":{"id":"7iazZR1BNIe9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cats = ['alt.atheism', 'sci.space']\n","newsgroup_train = fetch_20newsgroups(subset='train',\n","                                     categories=cats,\n","                                     remove=('headers', 'footers', 'quotes'))"],"metadata":{"id":"zgRwrp27NIcD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Number of news articles = {}'.format(len(newsgroup_train.data)))"],"metadata":{"id":"W-tF4GL5NIZT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763866269696,"user_tz":-330,"elapsed":18,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"ce22d248-3cab-46e4-aa85-c081aff74317"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of news articles = 1073\n"]}]},{"cell_type":"code","source":["norm_corpus = normalize_corpus(newsgroup_train.data)\n","norm_corpus"],"metadata":{"id":"SzBVF-4ENIWa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763866291119,"user_tz":-330,"elapsed":1185,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"outputId":"ab08fbc5-3b3d-4da8-a70b-16fbfb2d4567"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['please enlighten omnipotence contradictory definition occur universe governed rules nature thus god break anything god must allowed rules somewhere therefore omnipotence exist contradicts rules nature obviously omnipotent god change rules say definition exactly defined certainly omnipotence seem saying rules nature preexistant somehow define nature actually cause thats mean id like hear thoughts question',\n","       'aprkelvinjplnasagov baalkekelvinjplnasagov sorry think missed bit info transition experiment mean loss data magellan transmit data later btw nasa cut connection magellan looking forward day curious believe something funding goverment rather funding ok thats see guys around jurriaan',\n","       'henry made assumption gets firstest mostest wins ohhh want put fine print says thou shall wonderous rd rather use offtheshelf hardware sorry didnt see copy pournellesque proposals run along lines dollar amount reward simple goal go ahead development ill buy shelf higher cost even russian also assume thered buy us provos camped moon launching assembling little ittybitty payloads leo laser gas gun working bugs assembly integration leo oh hey could get couple canadarms tuned lunar environment wan na teleoperated prospecting im',\n","       ...,\n","       'publish plenty kiddoyou look sig files like strings every yoyos got one',\n","       'okay lets get record straight livermore gas gun project manager dr john hunter works laser group livermore may ask gas guns lasers nothing really gun physically located across road free electron laser building fel building heavily shielded control room thick walls gun firings controlled suspect office works administrative convenience visited hunter beginning feb toured gun time working gas gun rd boeing work things helping save space station gun uses methaneair mixture burned chamber ft long inch id ie looks like pipe chamber holds ton piston propelled several hundred ms chamber side piston hudrogen gas initially room temperature andsome tens atmospheres piston compresses heats hydrogen ahead stainless steel burst diaphragm ruptures around psi barrel gun feet long inch bore mounted right angles chamber ie intersect done future barrel could raised gun fired air without move larger heavier chamber projectile used testing kg cylinder lexan plastic diameter cm long acceleration comes expansion hydrogen gas psi downwards projectile leaves barrel barrel evacuated end sealed sheet plastic film little thicker saran wrap plastic blown small amount residual air trapped barrel ahead projectile gun fired bunker filled sandbags plastic water jugs early testing fragments plastic projectile found higher speeds later testing projectile vaporizes testing bunker livermore test range miles across projectile would go km fired maximum range intent move whole gun vandenberg afb testing complete fire pacific ocean use tracking radar vafb follow projectiles design goal gun throw kg projectile kms half orbital speed far reached kms gun currently repairs last test blew seal damaged hardware think methaneair detonating burning havent chance talk hunter directly people waiting test scramjet components gun firing gun air mach kms since get wind tunnels mach gun cost million develop basically proofofconcept bigger gun capable firing useful sized payloads space would require order kg projectiles deliver order kg useful payload orbit dani eder',\n","       'known quite earth actually pear shaped globularspherical anyone make globe accurate actual shape landmass configurationlonglat lines etc thanks advance billxpressouucp bill vance bothell wa rwingxpressobill'],\n","      dtype='<U32796')"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"id":"2QqMgDcVNITb","executionInfo":{"status":"ok","timestamp":1763866278575,"user_tz":-330,"elapsed":5768,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3be8a38c-3218-4815-d8c2-e75a53934b85"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n","Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n","Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: gensim\n","Successfully installed gensim-4.4.0\n"]}]},{"cell_type":"code","source":["from gensim.models import word2vec"],"metadata":{"id":"FgNgY9_0NIQz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenize_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]"],"metadata":{"id":"FTPOXVZgNIOI","executionInfo":{"status":"error","timestamp":1763866421415,"user_tz":-330,"elapsed":869,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"colab":{"base_uri":"https://localhost:8080/","height":141},"outputId":"56ad95db-3fda-4976-f4a3-56a1e815652e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'list' object has no attribute 'to'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4188211567.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnorm_corpus\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"]}]},{"cell_type":"code","source":["embedding_size = 32\n","context_window = 20\n","min_word_count = 1\n","sample = 1e-3\n","sg = 1\n","\n","w2v_model = word2vec.Word2Vec(tokenize_corpus,\n","                              vector_size = embedding_size,\n","                              window=context_window,\n","                              min_count = min_word_count,\n","                              sg = sg,\n","                              sample=sample,\n","                              epochs=200)"],"metadata":{"id":"JVi5QOBeNILT","executionInfo":{"status":"error","timestamp":1763866380074,"user_tz":-330,"elapsed":80256,"user":{"displayName":"Amar Jyoti","userId":"03450606962049459079"}},"colab":{"base_uri":"https://localhost:8080/","height":356},"outputId":"a270df19-5b8f-4731-cb73-a236a55dde6a"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1761558903.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m w2v_model = word2vec.Word2Vec(tokenize_corpus,\n\u001b[0m\u001b[1;32m      8\u001b[0m                               \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                               \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_window\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_corpus_sanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             self.train(\n\u001b[0m\u001b[1;32m    431\u001b[0m                 \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[1;32m   1074\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1432\u001b[0m             \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[1;32m   1435\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m             \u001b[0mreport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocks if workers too slow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# a thread reporting that it finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["print(f\"Unique numbers of words in the model={w2v_model.wv.vectors.shape[0]}\")"],"metadata":{"id":"afb2c8ZuNIIb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv['sun']"],"metadata":{"id":"hGOBuEsHNIEh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=['god'])"],"metadata":{"id":"EWYuTeVkNH4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["w2v_model.wv.most_similar(positive=['sun'])"],"metadata":{"id":"E3CAvdPaNWhc"},"execution_count":null,"outputs":[]}]}